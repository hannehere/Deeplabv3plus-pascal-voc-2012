{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6f8c388",
   "metadata": {},
   "source": [
    "# üèôÔ∏è DeepLabv3+ Cityscapes Reproduction on Kaggle\n",
    "\n",
    "## **Project Overview**\n",
    "This notebook reproduces **DeepLabv3+ semantic segmentation** results on the **Cityscapes dataset** using PyTorch and Torchvision. Adapted from our PASCAL VOC implementation with critical modifications for urban scene understanding.\n",
    "\n",
    "### **üîß Key Adaptations for Cityscapes:**\n",
    "- **NUM_CLASSES**: 19 (vs 21 for PASCAL VOC)\n",
    "- **RESOLUTION**: 769√ó769 (vs 513√ó513 for PASCAL VOC) \n",
    "- **LABEL MAPPING**: Critical remapping from labelIds ‚Üí trainIds\n",
    "- **TRAINING ITERATIONS**: 60,000 (vs 30,000 for PASCAL VOC)\n",
    "- **IGNORE INDEX**: 255 for unlabeled pixels\n",
    "\n",
    "### **üìä Dataset Info:**\n",
    "- **Training**: ~3,000 images (fine annotations)\n",
    "- **Validation**: ~500 images (fine annotations)\n",
    "- **Classes**: 19 semantic classes (road, sidewalk, building, wall, fence, pole, traffic light, traffic sign, vegetation, terrain, sky, person, rider, car, truck, bus, train, motorcycle, bicycle)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9690b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \udce5 Install Required Dependencies\n",
    "!pip install albumentations==1.3.1 kagglehub --quiet\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# üì¶ Import Essential Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.models.segmentation as models\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# üñºÔ∏è Image Processing & Augmentation\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# üìÅ File System & Utilities  \n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import kagglehub\n",
    "\n",
    "# \ud83düîß TPU v5e-8 Setup\n",
    "try:\n",
    "    # TPU Setup for Kaggle\n",
    "    import torch_xla\n",
    "    import torch_xla.core.xla_model as xm\n",
    "    import torch_xla.distributed.parallel_loader as pl\n",
    "    \n",
    "    device = xm.xla_device()\n",
    "    print(f\"üöÄ Using TPU v5e-8: {device}\")\n",
    "    \n",
    "    # Modern torch_xla API (v2.0+)\n",
    "    if hasattr(xm, 'xrt_world_size'):\n",
    "        # Legacy API\n",
    "        world_size = xm.xrt_world_size()\n",
    "        ordinal = xm.get_ordinal()\n",
    "    elif hasattr(xm, 'get_world_size'):\n",
    "        # Newer API\n",
    "        world_size = xm.get_world_size()\n",
    "        ordinal = xm.get_ordinal()\n",
    "    else:\n",
    "        # Latest API - use environment variables\n",
    "        import os\n",
    "        world_size = int(os.environ.get('WORLD_SIZE', '8'))\n",
    "        ordinal = int(os.environ.get('RANK', '0'))\n",
    "    \n",
    "    print(f\"   TPU Cores: {world_size}\")\n",
    "    print(f\"   Current Core: {ordinal}\")\n",
    "    \n",
    "    IS_TPU = True\n",
    "    \n",
    "except ImportError:\n",
    "    # Fallback to GPU/CPU\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
    "    print(f\"üöÄ TPU not available, using: {device}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "    \n",
    "    IS_TPU = False\n",
    "    world_size = 1  # Set fallback world_size for non-TPU\n",
    "    ordinal = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107d2940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üèôÔ∏è **CITYSCAPES CONFIGURATION** - Optimized for TPU v5e-8\n",
    "CFG = {\n",
    "    # üìä Dataset Configuration (CITYSCAPES SPECIFIC)\n",
    "    'NUM_CLASSES': 19,              # 19 semantic classes for Cityscapes (vs 21 for PASCAL VOC)\n",
    "    'IGNORE_INDEX': 255,            # Standard ignore index for unlabeled pixels\n",
    "    'CROP_SIZE': 769,               # Higher resolution for urban scenes (vs 513 for PASCAL VOC)  \n",
    "    'BASE_SIZE': 769,               # Base size for resize operations\n",
    "    \n",
    "    # üéØ Training Configuration (TPU v5e-8 OPTIMIZED)\n",
    "    'BATCH_SIZE': 32,               # Higher batch size leveraging TPU v5e-8 HBM (vs 2 for GPU)\n",
    "    'NUM_WORKERS': 8,               # More workers for TPU data loading efficiency\n",
    "    'MAX_ITERATIONS': 40000,        # Reduced due to larger effective batch size\n",
    "    'EVAL_INTERVAL': 1000,          # More frequent evaluation with faster TPU\n",
    "    'SAVE_INTERVAL': 2000,          # More frequent checkpoints\n",
    "    \n",
    "    # üîß Optimization Configuration (TPU TUNED)\n",
    "    'LEARNING_RATE': 0.08,          # Higher LR for larger batch size (linear scaling: 0.01 * 8)\n",
    "    'WEIGHT_DECAY': 5e-4,           # L2 regularization\n",
    "    'MOMENTUM': 0.9,                # SGD momentum\n",
    "    'POWER': 0.9,                   # Polynomial LR decay power\n",
    "    'WARMUP_ITERATIONS': 1000,      # LR warmup for large batch training\n",
    "    \n",
    "    # üìÅ Path Configuration (KAGGLE SPECIFIC)\n",
    "    'DATASET_ROOT': '/kaggle/input/cityscapes',  # Input dataset path\n",
    "    'OUTPUT_DIR': '/kaggle/working',              # Output directory for models/logs\n",
    "    \n",
    "    # üñºÔ∏è Data Augmentation Configuration\n",
    "    'RANDOM_SCALE_MIN': 0.5,        # Minimum scale for random scaling\n",
    "    'RANDOM_SCALE_MAX': 2.0,        # Maximum scale for random scaling  \n",
    "    'HORIZONTAL_FLIP_PROB': 0.5,    # Probability for horizontal flip\n",
    "    \n",
    "    # üß† Model Configuration (TPU OPTIMIZED)\n",
    "    'BACKBONE': 'resnet101',         # ResNet-101 backbone\n",
    "    'PRETRAINED': True,              # Use ImageNet pretrained weights\n",
    "    'MIXED_PRECISION': True,         # bfloat16 for TPU (instead of fp16)\n",
    "    'GRADIENT_ACCUMULATION': 1,      # No accumulation needed with larger batch size\n",
    "    'TPU_CORES': 8,                  # TPU v5e-8 has 8 cores\n",
    "    \n",
    "    # üìä ImageNet Normalization (Standard for pretrained models)\n",
    "    'MEAN': [0.485, 0.456, 0.406],\n",
    "    'STD': [0.229, 0.224, 0.225]\n",
    "}\n",
    "\n",
    "print(\"üèôÔ∏è **CITYSCAPES CONFIGURATION - TPU v5e-8 OPTIMIZED**\")\n",
    "print(f\"   Classes: {CFG['NUM_CLASSES']}\")\n",
    "print(f\"   Resolution: {CFG['CROP_SIZE']}√ó{CFG['CROP_SIZE']}\")  \n",
    "print(f\"   Batch Size: {CFG['BATCH_SIZE']} per core\")\n",
    "print(f\"   TPU Cores: {CFG['TPU_CORES']} (Total batch: {CFG['BATCH_SIZE'] * CFG['TPU_CORES']})\")\n",
    "print(f\"   Max Iterations: {CFG['MAX_ITERATIONS']:,}\")\n",
    "print(f\"   Learning Rate: {CFG['LEARNING_RATE']} (scaled for large batch)\")\n",
    "print(f\"   Dataset Path: {CFG['DATASET_ROOT']}\")\n",
    "print(f\"   Mixed Precision: {CFG['MIXED_PRECISION']} (bfloat16)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55463c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üè∑Ô∏è **CITYSCAPES LABEL MAPPING** - Critical for Correct Training\n",
    "\"\"\"\n",
    "Cityscapes uses complex label system:\n",
    "- labelIds: Original labels in *_labelIds.png files (0-33)  \n",
    "- trainIds: Training labels we need (0-18 + 255 for ignore)\n",
    "\n",
    "This mapping is ESSENTIAL for correct training!\n",
    "\"\"\"\n",
    "\n",
    "# üéØ Official Cityscapes Label Mapping (labelId -> trainId)\n",
    "CITYSCAPES_LABEL_MAP = {\n",
    "    # Road & Ground\n",
    "    7: 0,    # road\n",
    "    8: 1,    # sidewalk\n",
    "    11: 2,   # building\n",
    "    12: 3,   # wall\n",
    "    13: 4,   # fence\n",
    "    17: 5,   # pole\n",
    "    19: 6,   # traffic light\n",
    "    20: 7,   # traffic sign\n",
    "    21: 8,   # vegetation\n",
    "    22: 9,   # terrain\n",
    "    23: 10,  # sky\n",
    "    24: 11,  # person\n",
    "    25: 12,  # rider\n",
    "    26: 13,  # car\n",
    "    27: 14,  # truck\n",
    "    28: 15,  # bus\n",
    "    31: 16,  # train\n",
    "    32: 17,  # motorcycle\n",
    "    33: 18,  # bicycle\n",
    "}\n",
    "\n",
    "# üìã Class Names for Reference\n",
    "CITYSCAPES_CLASSES = [\n",
    "    'road', 'sidewalk', 'building', 'wall', 'fence', 'pole',\n",
    "    'traffic light', 'traffic sign', 'vegetation', 'terrain', 'sky',\n",
    "    'person', 'rider', 'car', 'truck', 'bus', 'train', 'motorcycle', 'bicycle'\n",
    "]\n",
    "\n",
    "def remap_labels(label_img):\n",
    "    \"\"\"\n",
    "    üîÑ Remap Cityscapes labelIds to trainIds\n",
    "    \n",
    "    Args:\n",
    "        label_img: numpy array with original labelIds\n",
    "    Returns:\n",
    "        remapped_img: numpy array with trainIds (0-18) and ignore_index (255)\n",
    "    \"\"\"\n",
    "    # Initialize with ignore index (255)\n",
    "    remapped = np.full_like(label_img, CFG['IGNORE_INDEX'], dtype=np.uint8)\n",
    "    \n",
    "    # Apply mapping for valid classes\n",
    "    for label_id, train_id in CITYSCAPES_LABEL_MAP.items():\n",
    "        mask = (label_img == label_id)\n",
    "        remapped[mask] = train_id\n",
    "    \n",
    "    return remapped\n",
    "\n",
    "# üß™ Test the mapping function\n",
    "print(\"üè∑Ô∏è **CITYSCAPES LABEL MAPPING LOADED**\")\n",
    "print(f\"   Valid Classes: {len(CITYSCAPES_CLASSES)}\")\n",
    "print(f\"   Label Mappings: {len(CITYSCAPES_LABEL_MAP)}\")\n",
    "print(f\"   Ignore Index: {CFG['IGNORE_INDEX']}\")\n",
    "print(\"\\nüìã **Class List:**\")\n",
    "for i, class_name in enumerate(CITYSCAPES_CLASSES):\n",
    "    print(f\"   {i:2d}: {class_name}\")\n",
    "    \n",
    "# Test mapping with dummy data\n",
    "test_labels = np.array([7, 24, 26, 0, 255])  # road, person, car, void, void\n",
    "test_remapped = remap_labels(test_labels)\n",
    "print(f\"\\nüß™ **Mapping Test:**\")\n",
    "print(f\"   Original: {test_labels}\")\n",
    "print(f\"   Remapped: {test_remapped}\")  # Should be [0, 11, 13, 255, 255]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270c109a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì• Download Cityscapes Dataset\n",
    "\"\"\"\n",
    "Download and setup Cityscapes dataset from Kaggle\n",
    "Expected structure:\n",
    "/kaggle/input/cityscapes/\n",
    "‚îú‚îÄ‚îÄ leftImg8bit/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ train/\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ val/\n",
    "‚îî‚îÄ‚îÄ gtFine/\n",
    "    ‚îú‚îÄ‚îÄ train/\n",
    "    ‚îî‚îÄ‚îÄ val/\n",
    "\"\"\"\n",
    "\n",
    "import kagglehub\n",
    "\n",
    "# Download Cityscapes dataset\n",
    "try:\n",
    "    print(\"üì• Downloading Cityscapes dataset...\")\n",
    "    \n",
    "    # Download latest version\n",
    "    path = kagglehub.dataset_download(\"dansbecker/cityscapes-image-pairs\")\n",
    "    print(\"Path to dataset files:\", path)\n",
    "    \n",
    "    # Update CFG with actual dataset path\n",
    "    CFG['DATASET_ROOT'] = path\n",
    "    print(f\"üîÑ Updated dataset root: {CFG['DATASET_ROOT']}\")\n",
    "    \n",
    "    # Verify dataset structure\n",
    "    if os.path.exists(path):\n",
    "        print(\"\\nüìÅ **Dataset Structure:**\")\n",
    "        for item in sorted(os.listdir(path)):\n",
    "            item_path = os.path.join(path, item)\n",
    "            if os.path.isdir(item_path):\n",
    "                print(f\"   üìÇ {item}/\")\n",
    "                # Show subdirectories\n",
    "                try:\n",
    "                    subdirs = [d for d in os.listdir(item_path) if os.path.isdir(os.path.join(item_path, d))]\n",
    "                    for subdir in sorted(subdirs)[:5]:  # Show first 5 subdirs\n",
    "                        print(f\"      üìÇ {subdir}/\")\n",
    "                        # Show sample files in first subdir\n",
    "                        if subdir == sorted(subdirs)[0]:\n",
    "                            subdir_path = os.path.join(item_path, subdir)\n",
    "                            files = [f for f in os.listdir(subdir_path) if os.path.isfile(os.path.join(subdir_path, f))]\n",
    "                            for file in sorted(files)[:3]:  # Show first 3 files\n",
    "                                print(f\"         üìÑ {file}\")\n",
    "                    if len(subdirs) > 5:\n",
    "                        print(f\"      ... and {len(subdirs)-5} more\")\n",
    "                except Exception as e:\n",
    "                    print(f\"      Error reading subdirs: {e}\")\n",
    "            else:\n",
    "                print(f\"   üìÑ {item}\")\n",
    "        \n",
    "        # Check for common Cityscapes patterns\n",
    "        possible_structures = [\n",
    "            \"leftImg8bit\",\n",
    "            \"gtFine\", \n",
    "            \"leftImg8bit_trainvaltest\",\n",
    "            \"gtFine_trainvaltest\",\n",
    "            \"images\",\n",
    "            \"labels\",\n",
    "            \"train\",\n",
    "            \"val\"\n",
    "        ]\n",
    "        \n",
    "        found_structures = []\n",
    "        for struct in possible_structures:\n",
    "            if os.path.exists(os.path.join(path, struct)):\n",
    "                found_structures.append(struct)\n",
    "        \n",
    "        if found_structures:\n",
    "            print(f\"\\nüîç **Found Cityscapes structures:** {found_structures}\")\n",
    "        else:\n",
    "            print(f\"\\n‚ö†Ô∏è  **Unknown dataset structure** - will try adaptive loading\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error downloading dataset: {e}\")\n",
    "    print(\"üí° Using default path: /kaggle/input/cityscapes\")\n",
    "    CFG['DATASET_ROOT'] = \"/kaggle/input/cityscapes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c6afeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üèôÔ∏è **CITYSCAPES DATASET CLASS** - Custom Dataset Implementation\n",
    "class CityscapesDataset(Dataset):\n",
    "    \"\"\"\n",
    "    üèôÔ∏è Custom Cityscapes Dataset for semantic segmentation\n",
    "    \n",
    "    Key Features:\n",
    "    - Handles leftImg8bit (RGB images) and gtFine (segmentation masks)\n",
    "    - Applies critical label remapping (labelIds -> trainIds)\n",
    "    - Supports different augmentations for train/val\n",
    "    - Optimized for TPU v5e-8 with efficient data loading\n",
    "    - Adaptive loading for different dataset structures\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, root_dir, split='train', transforms=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir: Path to cityscapes dataset\n",
    "            split: 'train' or 'val'\n",
    "            transforms: Albumentations transforms\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.split = split\n",
    "        self.transforms = transforms\n",
    "        \n",
    "        # üîç Find all image files with adaptive structure detection\n",
    "        self.image_files = []\n",
    "        self.label_files = []\n",
    "        \n",
    "        print(f\"üîç Searching for {split} images in: {root_dir}\")\n",
    "        \n",
    "        # Method 1: Standard Cityscapes structure\n",
    "        self.images_dir = os.path.join(root_dir, 'leftImg8bit', split)\n",
    "        self.labels_dir = os.path.join(root_dir, 'gtFine', split)\n",
    "        \n",
    "        if os.path.exists(self.images_dir) and os.path.exists(self.labels_dir):\n",
    "            print(f\"   ‚úÖ Found standard Cityscapes structure\")\n",
    "            self._load_standard_structure()\n",
    "        else:\n",
    "            print(f\"   ‚ö†Ô∏è  Standard structure not found, trying alternatives...\")\n",
    "            \n",
    "            # Method 2: Check for cityscapes_data subdirectory\n",
    "            cityscapes_subdir = os.path.join(root_dir, 'cityscapes_data')\n",
    "            if os.path.exists(cityscapes_subdir):\n",
    "                print(f\"   üîç Checking cityscapes_data subdirectory...\")\n",
    "                \n",
    "                # Try standard structure inside cityscapes_data\n",
    "                self.images_dir = os.path.join(cityscapes_subdir, 'leftImg8bit', split)\n",
    "                self.labels_dir = os.path.join(cityscapes_subdir, 'gtFine', split)\n",
    "                \n",
    "                if os.path.exists(self.images_dir) and os.path.exists(self.labels_dir):\n",
    "                    print(f\"   ‚úÖ Found standard structure in cityscapes_data/\")\n",
    "                    self._load_standard_structure()\n",
    "                else:\n",
    "                    # Try train/val folders directly in cityscapes_data\n",
    "                    split_dir = os.path.join(cityscapes_subdir, split)\n",
    "                    if os.path.exists(split_dir):\n",
    "                        print(f\"   üîç Found direct {split} folder in cityscapes_data/\")\n",
    "                        self._load_split_directory(split_dir)\n",
    "                    else:\n",
    "                        print(f\"   üîç Searching recursively in cityscapes_data/\")\n",
    "                        self._load_recursive_search(cityscapes_subdir)\n",
    "            else:\n",
    "                # Method 3: Check direct train/val folders\n",
    "                split_dir = os.path.join(root_dir, split)\n",
    "                if os.path.exists(split_dir):\n",
    "                    print(f\"   üîç Found direct {split} folder\")\n",
    "                    self._load_split_directory(split_dir)\n",
    "                else:\n",
    "                    # Method 4: Recursive search\n",
    "                    print(f\"   üîç Performing recursive search...\")\n",
    "                    self._load_recursive_search(root_dir)\n",
    "        \n",
    "        print(f\"üèôÔ∏è **Cityscapes {split.upper()} Dataset:**\")\n",
    "        print(f\"   Images: {len(self.image_files)}\")\n",
    "        print(f\"   Labels: {len(self.label_files)}\")\n",
    "        \n",
    "        if len(self.image_files) == 0:\n",
    "            print(\"‚ùå No images found! Debugging dataset structure...\")\n",
    "            self._debug_dataset_structure()\n",
    "        elif len(self.image_files) != len(self.label_files):\n",
    "            print(f\"‚ö†Ô∏è  Mismatch: {len(self.image_files)} images vs {len(self.label_files)} labels\")\n",
    "    \n",
    "    def _debug_dataset_structure(self):\n",
    "        \"\"\"Debug function to show what's actually in the dataset\"\"\"\n",
    "        print(\"üîç **DEBUGGING DATASET STRUCTURE:**\")\n",
    "        \n",
    "        def explore_directory(path, max_depth=3, current_depth=0):\n",
    "            if current_depth > max_depth or not os.path.exists(path):\n",
    "                return\n",
    "                \n",
    "            try:\n",
    "                items = sorted(os.listdir(path))[:20]  # Show first 20 items\n",
    "                indent = \"  \" * current_depth\n",
    "                \n",
    "                for item in items:\n",
    "                    item_path = os.path.join(path, item)\n",
    "                    if os.path.isdir(item_path):\n",
    "                        print(f\"{indent}üìÇ {item}/\")\n",
    "                        if current_depth < max_depth:\n",
    "                            explore_directory(item_path, max_depth, current_depth + 1)\n",
    "                    else:\n",
    "                        # Show file extension and size\n",
    "                        size = os.path.getsize(item_path)\n",
    "                        size_str = f\"({size/1024:.1f}KB)\" if size < 1024*1024 else f\"({size/1024/1024:.1f}MB)\"\n",
    "                        print(f\"{indent}üìÑ {item} {size_str}\")\n",
    "                        \n",
    "                if len(os.listdir(path)) > 20:\n",
    "                    print(f\"{indent}... and {len(os.listdir(path)) - 20} more items\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"{indent}‚ùå Error reading {path}: {e}\")\n",
    "        \n",
    "        print(f\"üìÅ Root directory: {self.root_dir}\")\n",
    "        explore_directory(self.root_dir)\n",
    "        \n",
    "        # Look for any image files\n",
    "        print(f\"\\nüîç **Searching for ANY image files:**\")\n",
    "        common_extensions = ['.png', '.jpg', '.jpeg']\n",
    "        found_images = []\n",
    "        \n",
    "        for root, dirs, files in os.walk(self.root_dir):\n",
    "            for file in files:\n",
    "                if any(file.lower().endswith(ext) for ext in common_extensions):\n",
    "                    found_images.append(os.path.join(root, file))\n",
    "                    if len(found_images) >= 10:  # Limit to first 10\n",
    "                        break\n",
    "            if len(found_images) >= 10:\n",
    "                break\n",
    "        \n",
    "        if found_images:\n",
    "            print(f\"üì∑ Found {len(found_images)} image files (showing first 10):\")\n",
    "            for img in found_images:\n",
    "                rel_path = os.path.relpath(img, self.root_dir)\n",
    "                print(f\"   üìÑ {rel_path}\")\n",
    "        else:\n",
    "            print(\"‚ùå No image files found anywhere in the dataset!\")\n",
    "    \n",
    "    def _load_standard_structure(self):\n",
    "        \"\"\"Load from standard Cityscapes structure with city folders\"\"\"\n",
    "        for city in sorted(os.listdir(self.images_dir)):\n",
    "            city_img_dir = os.path.join(self.images_dir, city)\n",
    "            city_lbl_dir = os.path.join(self.labels_dir, city)\n",
    "            \n",
    "            if os.path.isdir(city_img_dir):\n",
    "                # Get image files\n",
    "                img_files = glob.glob(os.path.join(city_img_dir, '*_leftImg8bit.png'))\n",
    "                \n",
    "                for img_file in sorted(img_files):\n",
    "                    # Corresponding label file\n",
    "                    basename = os.path.basename(img_file).replace('_leftImg8bit.png', '')\n",
    "                    label_file = os.path.join(city_lbl_dir, f'{basename}_gtFine_labelIds.png')\n",
    "                    \n",
    "                    if os.path.exists(label_file):\n",
    "                        self.image_files.append(img_file)\n",
    "                        self.label_files.append(label_file)\n",
    "    \n",
    "    def _load_split_directory(self, split_dir):\n",
    "        \"\"\"Load from a split directory (train/ or val/)\"\"\"\n",
    "        print(f\"   üìÅ Scanning directory: {split_dir}\")\n",
    "        \n",
    "        # Look for image and label patterns\n",
    "        all_files = []\n",
    "        for root, dirs, files in os.walk(split_dir):\n",
    "            for file in files:\n",
    "                if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                    all_files.append(os.path.join(root, file))\n",
    "        \n",
    "        print(f\"   üìÑ Found {len(all_files)} image files\")\n",
    "        \n",
    "        # Separate images and labels based on filename patterns\n",
    "        potential_images = []\n",
    "        potential_labels = []\n",
    "        \n",
    "        for file_path in all_files:\n",
    "            filename = os.path.basename(file_path).lower()\n",
    "            \n",
    "            # Image patterns\n",
    "            if any(pattern in filename for pattern in ['leftimg8bit', 'img', 'image', 'rgb']):\n",
    "                potential_images.append(file_path)\n",
    "            # Label patterns  \n",
    "            elif any(pattern in filename for pattern in ['labelids', 'gtfine', 'label', 'mask', 'seg']):\n",
    "                potential_labels.append(file_path)\n",
    "            # If no clear pattern, assume images are larger files\n",
    "            else:\n",
    "                try:\n",
    "                    size = os.path.getsize(file_path)\n",
    "                    if size > 50000:  # Assume files > 50KB are images\n",
    "                        potential_images.append(file_path)\n",
    "                    else:\n",
    "                        potential_labels.append(file_path)\n",
    "                except:\n",
    "                    potential_images.append(file_path)  # Default to image\n",
    "        \n",
    "        print(f\"   üñºÔ∏è  Potential images: {len(potential_images)}\")\n",
    "        print(f\"   üè∑Ô∏è  Potential labels: {len(potential_labels)}\")\n",
    "        \n",
    "        # Show some examples\n",
    "        if potential_images:\n",
    "            print(f\"   üìù Image examples: {[os.path.basename(f) for f in potential_images[:3]]}\")\n",
    "        if potential_labels:\n",
    "            print(f\"   \udcdd Label examples: {[os.path.basename(f) for f in potential_labels[:3]]}\")\n",
    "        \n",
    "        # Match images with labels\n",
    "        for img_file in sorted(potential_images):\n",
    "            img_basename = os.path.basename(img_file)\n",
    "            \n",
    "            # Try to find corresponding label\n",
    "            best_label = None\n",
    "            \n",
    "            # Method 1: Try exact matching patterns\n",
    "            for label_file in potential_labels:\n",
    "                label_basename = os.path.basename(label_file)\n",
    "                \n",
    "                # Remove common suffixes and compare\n",
    "                img_core = img_basename.lower().replace('_leftimg8bit', '').replace('_img', '').replace('_image', '').split('.')[0]\n",
    "                label_core = label_basename.lower().replace('_gtfine_labelids', '').replace('_label', '').replace('_mask', '').split('.')[0]\n",
    "                \n",
    "                if img_core == label_core:\n",
    "                    best_label = label_file\n",
    "                    break\n",
    "            \n",
    "            # Method 2: If no exact match, try same directory\n",
    "            if not best_label and potential_labels:\n",
    "                img_dir = os.path.dirname(img_file)\n",
    "                for label_file in potential_labels:\n",
    "                    if os.path.dirname(label_file) == img_dir:\n",
    "                        best_label = label_file\n",
    "                        break\n",
    "            \n",
    "            # Method 3: Use first available label (fallback)\n",
    "            if not best_label and potential_labels:\n",
    "                best_label = potential_labels[0]\n",
    "                potential_labels.remove(best_label)  # Remove so it's not reused\n",
    "            \n",
    "            if best_label:\n",
    "                self.image_files.append(img_file)\n",
    "                self.label_files.append(best_label)\n",
    "    \n",
    "    def _load_recursive_search(self, search_dir):\n",
    "        \"\"\"Recursive search for image-label pairs\"\"\"\n",
    "        print(f\"   üîç Recursive search in: {search_dir}\")\n",
    "        self._load_split_directory(search_dir)  # Use same logic as split directory\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get image and label pair with preprocessing\n",
    "        \"\"\"\n",
    "        if idx >= len(self.image_files):\n",
    "            raise IndexError(f\"Index {idx} out of range for dataset of size {len(self.image_files)}\")\n",
    "        \n",
    "        # üìñ Load image and label\n",
    "        image_path = self.image_files[idx] \n",
    "        label_path = self.label_files[idx]\n",
    "        \n",
    "        try:\n",
    "            # Load image (RGB)\n",
    "            image = cv2.imread(image_path)\n",
    "            if image is None:\n",
    "                raise ValueError(f\"Could not load image: {image_path}\")\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Load label (grayscale)\n",
    "            label = cv2.imread(label_path, cv2.IMREAD_GRAYSCALE)\n",
    "            if label is None:\n",
    "                raise ValueError(f\"Could not load label: {label_path}\")\n",
    "            \n",
    "            # üîÑ Critical: Remap labels (labelIds -> trainIds)\n",
    "            label = remap_labels(label)\n",
    "            \n",
    "            # üñºÔ∏è Apply transforms\n",
    "            if self.transforms:\n",
    "                transformed = self.transforms(image=image, mask=label)\n",
    "                image = transformed['image']\n",
    "                label = transformed['mask']\n",
    "            \n",
    "            return image, label.long()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading sample {idx}: {e}\")\n",
    "            print(f\"   Image path: {image_path}\")\n",
    "            print(f\"   Label path: {label_path}\")\n",
    "            raise\n",
    "    \n",
    "    def get_sample_info(self, idx):\n",
    "        \"\"\"Get file paths for debugging\"\"\"\n",
    "        return {\n",
    "            'image_path': self.image_files[idx] if idx < len(self.image_files) else None,\n",
    "            'label_path': self.label_files[idx] if idx < len(self.label_files) else None\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25d721a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîç **QUICK DATASET STRUCTURE INSPECTION**\n",
    "\"\"\"\n",
    "Let's examine what's actually in the downloaded Cityscapes dataset\n",
    "\"\"\"\n",
    "\n",
    "def inspect_dataset_structure(root_path, max_files=10):\n",
    "    \"\"\"Inspect and display dataset structure\"\"\"\n",
    "    print(f\"üîç **INSPECTING DATASET:** {root_path}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    if not os.path.exists(root_path):\n",
    "        print(f\"‚ùå Path does not exist: {root_path}\")\n",
    "        return\n",
    "    \n",
    "    def explore_path(path, depth=0, max_depth=4):\n",
    "        if depth > max_depth:\n",
    "            return\n",
    "            \n",
    "        indent = \"  \" * depth\n",
    "        try:\n",
    "            items = os.listdir(path)\n",
    "            dirs = [item for item in items if os.path.isdir(os.path.join(path, item))]\n",
    "            files = [item for item in items if os.path.isfile(os.path.join(path, item))]\n",
    "            \n",
    "            # Show directories first\n",
    "            for dir_name in sorted(dirs)[:10]:  # Limit to 10 dirs\n",
    "                dir_path = os.path.join(path, dir_name)\n",
    "                file_count = 0\n",
    "                try:\n",
    "                    file_count = len([f for f in os.listdir(dir_path) if os.path.isfile(os.path.join(dir_path, f))])\n",
    "                except:\n",
    "                    pass\n",
    "                print(f\"{indent}üìÇ {dir_name}/ ({file_count} files)\")\n",
    "                if depth < max_depth:\n",
    "                    explore_path(dir_path, depth + 1, max_depth)\n",
    "            \n",
    "            # Show some sample files\n",
    "            if files and depth <= 2:\n",
    "                image_files = [f for f in files if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "                other_files = [f for f in files if not f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "                \n",
    "                if image_files:\n",
    "                    print(f\"{indent}üñºÔ∏è  Image files ({len(image_files)} total):\")\n",
    "                    for file_name in sorted(image_files)[:max_files]:\n",
    "                        file_path = os.path.join(path, file_name)\n",
    "                        size = os.path.getsize(file_path) / 1024  # KB\n",
    "                        print(f\"{indent}   üìÑ {file_name} ({size:.1f}KB)\")\n",
    "                    if len(image_files) > max_files:\n",
    "                        print(f\"{indent}   ... and {len(image_files) - max_files} more image files\")\n",
    "                \n",
    "                if other_files and len(other_files) <= 20:\n",
    "                    print(f\"{indent}üìÑ Other files: {other_files}\")\n",
    "                elif other_files:\n",
    "                    print(f\"{indent}üìÑ Other files ({len(other_files)} total): {other_files[:5]}...\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"{indent}‚ùå Error reading {path}: {e}\")\n",
    "    \n",
    "    explore_path(root_path)\n",
    "    \n",
    "    # Search for specific Cityscapes patterns\n",
    "    print(f\"\\nüîç **SEARCHING FOR CITYSCAPES PATTERNS:**\")\n",
    "    cityscapes_patterns = ['leftImg8bit', 'gtFine', 'labelIds', 'train', 'val']\n",
    "    \n",
    "    for pattern in cityscapes_patterns:\n",
    "        found_paths = []\n",
    "        for root, dirs, files in os.walk(root_path):\n",
    "            # Check directories\n",
    "            for dir_name in dirs:\n",
    "                if pattern.lower() in dir_name.lower():\n",
    "                    found_paths.append(os.path.join(root, dir_name))\n",
    "            # Check files  \n",
    "            for file_name in files:\n",
    "                if pattern.lower() in file_name.lower():\n",
    "                    found_paths.append(os.path.join(root, file_name))\n",
    "                    if len(found_paths) >= 5:  # Limit results\n",
    "                        break\n",
    "            if len(found_paths) >= 5:\n",
    "                break\n",
    "        \n",
    "        if found_paths:\n",
    "            print(f\"   üéØ '{pattern}' found in:\")\n",
    "            for path in found_paths[:5]:\n",
    "                rel_path = os.path.relpath(path, root_path)\n",
    "                print(f\"      üìç {rel_path}\")\n",
    "        else:\n",
    "            print(f\"   ‚ùå '{pattern}' not found\")\n",
    "\n",
    "# Inspect the actual dataset\n",
    "if 'CFG' in globals() and 'DATASET_ROOT' in CFG:\n",
    "    inspect_dataset_structure(CFG['DATASET_ROOT'])\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è CFG not found, using default path\")\n",
    "    inspect_dataset_structure(\"/kaggle/input/cityscapes-image-pairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085d4f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üéØ **CITYSCAPES DATASET FIX** - Handle Specific Structure\n",
    "\"\"\"\n",
    "Fix the dataset loading for the specific structure we found:\n",
    "cityscapes_data/train/ and cityscapes_data/val/\n",
    "\"\"\"\n",
    "\n",
    "def quick_test_dataset_loading():\n",
    "    \"\"\"Test dataset loading with the actual structure\"\"\"\n",
    "    dataset_root = CFG['DATASET_ROOT']\n",
    "    \n",
    "    print(f\"üß™ **TESTING DATASET LOADING:**\")\n",
    "    print(f\"   Dataset root: {dataset_root}\")\n",
    "    \n",
    "    # Check the specific paths we know exist\n",
    "    cityscapes_data_dir = os.path.join(dataset_root, 'cityscapes_data')\n",
    "    train_dir = os.path.join(cityscapes_data_dir, 'train')\n",
    "    val_dir = os.path.join(cityscapes_data_dir, 'val')\n",
    "    \n",
    "    print(f\"   Cityscapes data dir exists: {os.path.exists(cityscapes_data_dir)}\")\n",
    "    print(f\"   Train dir exists: {os.path.exists(train_dir)}\")\n",
    "    print(f\"   Val dir exists: {os.path.exists(val_dir)}\")\n",
    "    \n",
    "    # Count files in each directory\n",
    "    for split, split_dir in [('train', train_dir), ('val', val_dir)]:\n",
    "        if os.path.exists(split_dir):\n",
    "            all_files = []\n",
    "            for root, dirs, files in os.walk(split_dir):\n",
    "                for file in files:\n",
    "                    if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                        all_files.append(os.path.join(root, file))\n",
    "            \n",
    "            print(f\"   {split.capitalize()} directory: {len(all_files)} image files\")\n",
    "            \n",
    "            # Show some examples\n",
    "            if all_files:\n",
    "                print(f\"   {split.capitalize()} examples:\")\n",
    "                for i, file_path in enumerate(sorted(all_files)[:5]):\n",
    "                    filename = os.path.basename(file_path)\n",
    "                    size = os.path.getsize(file_path) / 1024  # KB\n",
    "                    print(f\"      {i+1}. {filename} ({size:.1f}KB)\")\n",
    "                \n",
    "                # Try to identify image/label patterns\n",
    "                potential_images = []\n",
    "                potential_labels = []\n",
    "                \n",
    "                for file_path in all_files:\n",
    "                    filename = os.path.basename(file_path).lower()\n",
    "                    size = os.path.getsize(file_path)\n",
    "                    \n",
    "                    # Classify based on filename patterns and size\n",
    "                    if 'label' in filename or 'mask' in filename or 'seg' in filename:\n",
    "                        potential_labels.append(file_path)\n",
    "                    elif 'img' in filename or 'image' in filename or size > 100000:  # > 100KB likely images\n",
    "                        potential_images.append(file_path)\n",
    "                    else:\n",
    "                        # Default classification by size\n",
    "                        if size > 50000:  # > 50KB\n",
    "                            potential_images.append(file_path)\n",
    "                        else:\n",
    "                            potential_labels.append(file_path)\n",
    "                \n",
    "                print(f\"   Classified as images: {len(potential_images)}\")\n",
    "                print(f\"   Classified as labels: {len(potential_labels)}\")\n",
    "                \n",
    "                if potential_images:\n",
    "                    print(f\"   Sample images: {[os.path.basename(f) for f in potential_images[:3]]}\")\n",
    "                if potential_labels:\n",
    "                    print(f\"   Sample labels: {[os.path.basename(f) for f in potential_labels[:3]]}\")\n",
    "    \n",
    "    print(\"\\nüîß Now let's test the actual dataset class...\")\n",
    "    \n",
    "    # Test with a simple dataset creation\n",
    "    try:\n",
    "        # Use the enhanced dataset path\n",
    "        enhanced_root = cityscapes_data_dir  # Point directly to cityscapes_data\n",
    "        \n",
    "        print(f\"üß™ Testing dataset with root: {enhanced_root}\")\n",
    "        \n",
    "        # Test train dataset\n",
    "        test_train_dataset = CityscapesDataset(\n",
    "            root_dir=enhanced_root,\n",
    "            split='train',\n",
    "            transforms=None  # No transforms for testing\n",
    "        )\n",
    "        \n",
    "        # Test val dataset  \n",
    "        test_val_dataset = CityscapesDataset(\n",
    "            root_dir=enhanced_root,\n",
    "            split='val',\n",
    "            transforms=None\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n‚úÖ **DATASET TEST RESULTS:**\")\n",
    "        print(f\"   Train samples: {len(test_train_dataset)}\")\n",
    "        print(f\"   Val samples: {len(test_val_dataset)}\")\n",
    "        \n",
    "        if len(test_train_dataset) > 0:\n",
    "            print(f\"   ‚úÖ Successfully found training data!\")\n",
    "            # Test loading one sample\n",
    "            try:\n",
    "                sample_img, sample_label = test_train_dataset[0]\n",
    "                print(f\"   Sample image shape: {sample_img.shape}\")\n",
    "                print(f\"   Sample label shape: {sample_label.shape}\")\n",
    "                print(f\"   Sample label unique values: {torch.unique(sample_label).tolist()}\")\n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ö†Ô∏è Error loading sample: {e}\")\n",
    "        \n",
    "        # Update the global CFG to use the correct path\n",
    "        if len(test_train_dataset) > 0 or len(test_val_dataset) > 0:\n",
    "            CFG['DATASET_ROOT'] = enhanced_root\n",
    "            print(f\"\\nüîÑ Updated CFG['DATASET_ROOT'] to: {CFG['DATASET_ROOT']}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"\\n‚ùå No data found even with enhanced path\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Dataset test failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "\n",
    "# Run the test\n",
    "success = quick_test_dataset_loading()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869fd62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üñºÔ∏è **DATA AUGMENTATION PIPELINE** - Cityscapes Specific\n",
    "\"\"\"\n",
    "Augmentation strategy adapted for Cityscapes:\n",
    "- Higher resolution (769x769) for urban scene detail\n",
    "- Scale range 0.5-2.0 for variety\n",
    "- Careful padding with ignore_index for masks\n",
    "\"\"\"\n",
    "\n",
    "def get_train_transforms():\n",
    "    \"\"\"üèãÔ∏è Training augmentations for Cityscapes\"\"\"\n",
    "    return A.Compose([\n",
    "        # üìè Random scaling (0.5x to 2.0x)\n",
    "        A.RandomScale(scale_limit=(CFG['RANDOM_SCALE_MIN']-1, CFG['RANDOM_SCALE_MAX']-1), \n",
    "                      interpolation=cv2.INTER_LINEAR, p=1.0),\n",
    "        \n",
    "        # üìê Pad if needed to ensure minimum size\n",
    "        A.PadIfNeeded(min_height=CFG['CROP_SIZE'], min_width=CFG['CROP_SIZE'],\n",
    "                      border_mode=cv2.BORDER_CONSTANT, value=0, mask_value=CFG['IGNORE_INDEX']),\n",
    "        \n",
    "        # ‚úÇÔ∏è Random crop to target size\n",
    "        A.RandomCrop(height=CFG['CROP_SIZE'], width=CFG['CROP_SIZE']),\n",
    "        \n",
    "        # üîÑ Horizontal flip\n",
    "        A.HorizontalFlip(p=CFG['HORIZONTAL_FLIP_PROB']),\n",
    "        \n",
    "        # üé® Color augmentations (optional - comment out if too aggressive)\n",
    "        A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1, p=0.3),\n",
    "        \n",
    "        # üìä Normalization & tensor conversion\n",
    "        A.Normalize(mean=CFG['MEAN'], std=CFG['STD']),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "\n",
    "def get_val_transforms():\n",
    "    \"\"\"üß™ Validation transforms (minimal processing)\"\"\"\n",
    "    return A.Compose([\n",
    "        # üìè Resize to target size (can use different strategy)\n",
    "        A.Resize(height=CFG['CROP_SIZE'], width=CFG['CROP_SIZE'], \n",
    "                 interpolation=cv2.INTER_LINEAR),\n",
    "        \n",
    "        # üìä Normalization & tensor conversion  \n",
    "        A.Normalize(mean=CFG['MEAN'], std=CFG['STD']),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "\n",
    "# üß™ Test transforms\n",
    "print(\"üñºÔ∏è **AUGMENTATION PIPELINE READY**\")\n",
    "print(f\"   Crop Size: {CFG['CROP_SIZE']}√ó{CFG['CROP_SIZE']}\")\n",
    "print(f\"   Scale Range: {CFG['RANDOM_SCALE_MIN']:.1f} - {CFG['RANDOM_SCALE_MAX']:.1f}\")\n",
    "print(f\"   Horizontal Flip: {CFG['HORIZONTAL_FLIP_PROB']*100:.0f}%\")\n",
    "print(f\"   Ignore Index: {CFG['IGNORE_INDEX']} (for padding)\")\n",
    "\n",
    "# Test with dummy data\n",
    "test_img = np.random.randint(0, 255, (1024, 2048, 3), dtype=np.uint8)\n",
    "test_mask = np.random.randint(0, 19, (1024, 2048), dtype=np.uint8)\n",
    "\n",
    "train_transform = get_train_transforms()\n",
    "val_transform = get_val_transforms()\n",
    "\n",
    "try:\n",
    "    # Test transforms\n",
    "    train_result = train_transform(image=test_img, mask=test_mask)\n",
    "    val_result = val_transform(image=test_img, mask=test_mask)\n",
    "    \n",
    "    print(f\"\\n‚úÖ **Transform Test Passed:**\")\n",
    "    print(f\"   Train Output: {train_result['image'].shape}, {train_result['mask'].shape}\")\n",
    "    print(f\"   Val Output: {val_result['image'].shape}, {val_result['mask'].shape}\")\n",
    "    print(f\"   Train Image Range: [{train_result['image'].min():.3f}, {train_result['image'].max():.3f}]\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Transform Test Failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcaf3db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß† **DEEPLABV3+ MODEL CREATION** - TPU Optimized\n",
    "\"\"\"\n",
    "Create DeepLabv3+ model with ResNet-101 backbone\n",
    "Key modifications for Cityscapes:\n",
    "- NUM_CLASSES = 19 (vs 21 for PASCAL VOC)\n",
    "- TPU-optimized with bfloat16 mixed precision\n",
    "- Proper classifier head modification\n",
    "\"\"\"\n",
    "\n",
    "def create_deeplabv3plus_model():\n",
    "    \"\"\"\n",
    "    üèóÔ∏è Create DeepLabv3+ model for Cityscapes\n",
    "    \"\"\"\n",
    "    print(\"üß† Creating DeepLabv3+ model...\")\n",
    "    \n",
    "    # Load pretrained DeepLabv3+ with ResNet-101\n",
    "    model = models.deeplabv3_resnet101(\n",
    "        pretrained=CFG['PRETRAINED'],\n",
    "        progress=True,\n",
    "        num_classes=21  # Start with PASCAL VOC pretrained\n",
    "    )\n",
    "    \n",
    "    # üîß Modify classifier for Cityscapes (19 classes)\n",
    "    # DeepLabv3+ has classifier and aux_classifier\n",
    "    model.classifier[4] = nn.Conv2d(\n",
    "        in_channels=256,\n",
    "        out_channels=CFG['NUM_CLASSES'], \n",
    "        kernel_size=1\n",
    "    )\n",
    "    \n",
    "    if hasattr(model, 'aux_classifier') and model.aux_classifier is not None:\n",
    "        model.aux_classifier[4] = nn.Conv2d(\n",
    "            in_channels=256,\n",
    "            out_channels=CFG['NUM_CLASSES'],\n",
    "            kernel_size=1\n",
    "        )\n",
    "    \n",
    "    print(f\"‚úÖ Model created with {CFG['NUM_CLASSES']} classes\")\n",
    "    \n",
    "    # üìä Model info\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(f\"   Total parameters: {total_params:,}\")\n",
    "    print(f\"   Trainable parameters: {trainable_params:,}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# üèóÔ∏è Create model\n",
    "model = create_deeplabv3plus_model()\n",
    "\n",
    "# \udd0d Check TPU availability (from previous cell)\n",
    "try:\n",
    "    import torch_xla.core.xla_model as xm\n",
    "    IS_TPU = True\n",
    "    device = xm.xla_device()\n",
    "except ImportError:\n",
    "    IS_TPU = False\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# \ud83düöÄ Move to TPU device\n",
    "if IS_TPU:\n",
    "    model = model.to(device)\n",
    "    print(f\"üì± Model moved to TPU: {device}\")\n",
    "else:\n",
    "    model = model.to(device)\n",
    "    print(f\"üì± Model moved to device: {device}\")\n",
    "\n",
    "# üß™ Test model with dummy input\n",
    "try:\n",
    "    dummy_input = torch.randn(1, 3, CFG['CROP_SIZE'], CFG['CROP_SIZE']).to(device)\n",
    "    \n",
    "    if IS_TPU:\n",
    "        # TPU requires different handling\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            output = model(dummy_input)\n",
    "    else:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            output = model(dummy_input)\n",
    "    \n",
    "    print(f\"‚úÖ **Model Test Passed:**\")\n",
    "    print(f\"   Input shape: {dummy_input.shape}\")\n",
    "    print(f\"   Output shape: {output['out'].shape}\")\n",
    "    if 'aux' in output:\n",
    "        print(f\"   Aux output shape: {output['aux'].shape}\")\n",
    "    \n",
    "    # Verify output channels\n",
    "    expected_shape = (1, CFG['NUM_CLASSES'], CFG['CROP_SIZE'], CFG['CROP_SIZE'])\n",
    "    if output['out'].shape == expected_shape:\n",
    "        print(f\"   ‚úÖ Output shape correct: {expected_shape}\")\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è  Output shape mismatch: expected {expected_shape}, got {output['out'].shape}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Model test failed: {e}\")\n",
    "\n",
    "model.train()  # Set back to training mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe88ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä **DATASET CREATION & TPU DATALOADER** - Optimized Pipeline\n",
    "\"\"\"\n",
    "Create train/val datasets and TPU-optimized DataLoaders\n",
    "Key features:\n",
    "- CityscapesDataset with label remapping\n",
    "- TPU-aware data loading\n",
    "- Parallel data loading across cores\n",
    "\"\"\"\n",
    "\n",
    "# üì• Check dataset availability first\n",
    "if not os.path.exists(CFG['DATASET_ROOT']):\n",
    "    print(f\"‚ùå Dataset not found at: {CFG['DATASET_ROOT']}\")\n",
    "    print(\"üí° Creating dummy dataset for testing...\")\n",
    "    \n",
    "    # Create minimal dummy structure for testing\n",
    "    dummy_root = \"/tmp/cityscapes_dummy\"\n",
    "    os.makedirs(os.path.join(dummy_root, \"leftImg8bit\", \"train\"), exist_ok=True)\n",
    "    os.makedirs(os.path.join(dummy_root, \"leftImg8bit\", \"val\"), exist_ok=True)\n",
    "    os.makedirs(os.path.join(dummy_root, \"gtFine\", \"train\"), exist_ok=True)\n",
    "    os.makedirs(os.path.join(dummy_root, \"gtFine\", \"val\"), exist_ok=True)\n",
    "    \n",
    "    # Create a few dummy files for testing\n",
    "    import cv2\n",
    "    for split in ['train', 'val']:\n",
    "        for i in range(2):  # Just 2 samples each\n",
    "            # Dummy image\n",
    "            dummy_img = np.random.randint(0, 255, (512, 1024, 3), dtype=np.uint8)\n",
    "            img_path = os.path.join(dummy_root, \"leftImg8bit\", split, f\"sample_{i:03d}_leftImg8bit.png\")\n",
    "            cv2.imwrite(img_path, dummy_img)\n",
    "            \n",
    "            # Dummy label\n",
    "            dummy_label = np.random.randint(7, 34, (512, 1024), dtype=np.uint8)  # Use valid labelIds\n",
    "            label_path = os.path.join(dummy_root, \"gtFine\", split, f\"sample_{i:03d}_gtFine_labelIds.png\")\n",
    "            cv2.imwrite(label_path, dummy_label)\n",
    "    \n",
    "    CFG['DATASET_ROOT'] = dummy_root\n",
    "    print(f\"‚úÖ Dummy dataset created at: {CFG['DATASET_ROOT']}\")\n",
    "\n",
    "# üèóÔ∏è Create datasets\n",
    "print(\"üìä Creating Cityscapes datasets...\")\n",
    "\n",
    "train_dataset = CityscapesDataset(\n",
    "    root_dir=CFG['DATASET_ROOT'],\n",
    "    split='train',\n",
    "    transforms=get_train_transforms()\n",
    ")\n",
    "\n",
    "val_dataset = CityscapesDataset(\n",
    "    root_dir=CFG['DATASET_ROOT'], \n",
    "    split='val',\n",
    "    transforms=get_val_transforms()\n",
    ")\n",
    "\n",
    "print(f\"\\nüìà **Dataset Summary:**\")\n",
    "print(f\"   Training samples: {len(train_dataset):,}\")\n",
    "print(f\"   Validation samples: {len(val_dataset):,}\")\n",
    "print(f\"   Total samples: {len(train_dataset) + len(val_dataset):,}\")\n",
    "\n",
    "# üîç Get TPU/device info (from previous cells)\n",
    "try:\n",
    "    import torch_xla.core.xla_model as xm\n",
    "    IS_TPU = True\n",
    "    device = xm.xla_device()\n",
    "    \n",
    "    # Get world_size and ordinal with modern API detection\n",
    "    if hasattr(xm, 'xrt_world_size'):\n",
    "        world_size = xm.xrt_world_size()\n",
    "        ordinal = xm.get_ordinal()\n",
    "    elif hasattr(xm, 'get_world_size'):\n",
    "        world_size = xm.get_world_size()\n",
    "        ordinal = xm.get_ordinal()\n",
    "    else:\n",
    "        import os\n",
    "        world_size = int(os.environ.get('WORLD_SIZE', '8'))\n",
    "        ordinal = int(os.environ.get('RANK', '0'))\n",
    "        \n",
    "except ImportError:\n",
    "    IS_TPU = False\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    world_size = 1\n",
    "    ordinal = 0\n",
    "\n",
    "# üöÄ Create TPU-optimized DataLoaders\n",
    "if IS_TPU:\n",
    "    # TPU requires special data loading\n",
    "    print(\"üöÄ Creating TPU DataLoaders...\")\n",
    "    \n",
    "    # Use the world_size and ordinal variables\n",
    "    print(f\"   Using world_size: {world_size}, ordinal: {ordinal}\")\n",
    "    \n",
    "    train_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "        train_dataset,\n",
    "        num_replicas=world_size,\n",
    "        rank=ordinal,\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    val_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "        val_dataset,\n",
    "        num_replicas=world_size,\n",
    "        rank=ordinal,\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=CFG['BATCH_SIZE'],\n",
    "        sampler=train_sampler,\n",
    "        num_workers=CFG['NUM_WORKERS'],\n",
    "        drop_last=True,\n",
    "        pin_memory=False  # Not needed for TPU\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=CFG['BATCH_SIZE'],\n",
    "        sampler=val_sampler,\n",
    "        num_workers=CFG['NUM_WORKERS'],\n",
    "        drop_last=False,\n",
    "        pin_memory=False\n",
    "    )\n",
    "    \n",
    "    # Wrap with TPU ParallelLoader\n",
    "    import torch_xla.distributed.parallel_loader as pl\n",
    "    train_loader = pl.MpDeviceLoader(train_loader, device)\n",
    "    val_loader = pl.MpDeviceLoader(val_loader, device)\n",
    "    \n",
    "    effective_batch_size = CFG['BATCH_SIZE'] * world_size\n",
    "    print(f\"   TPU Cores: {world_size}\")\n",
    "    print(f\"   Batch per core: {CFG['BATCH_SIZE']}\")\n",
    "    print(f\"   Effective batch size: {effective_batch_size}\")\n",
    "    \n",
    "else:\n",
    "    # Regular GPU/CPU DataLoaders\n",
    "    print(\"üíª Creating GPU/CPU DataLoaders...\")\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=CFG['BATCH_SIZE'],\n",
    "        shuffle=True,\n",
    "        num_workers=CFG['NUM_WORKERS'],\n",
    "        drop_last=True,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=CFG['BATCH_SIZE'],\n",
    "        shuffle=False,\n",
    "        num_workers=CFG['NUM_WORKERS'],\n",
    "        drop_last=False,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    effective_batch_size = CFG['BATCH_SIZE']\n",
    "    print(f\"   Batch size: {effective_batch_size}\")\n",
    "\n",
    "print(f\"   Training batches: {len(train_loader):,}\")\n",
    "print(f\"   Validation batches: {len(val_loader):,}\")\n",
    "\n",
    "# üß™ Test data loading\n",
    "if len(train_dataset) > 0:\n",
    "    print(\"\\nüß™ Testing data loading...\")\n",
    "    try:\n",
    "        # Get one batch\n",
    "        data_iter = iter(train_loader)\n",
    "        images, labels = next(data_iter)\n",
    "        \n",
    "        print(f\"‚úÖ **Data Loading Test Passed:**\")\n",
    "        print(f\"   Batch images shape: {images.shape}\")\n",
    "        print(f\"   Batch labels shape: {labels.shape}\")\n",
    "        print(f\"   Image dtype: {images.dtype}\")\n",
    "        print(f\"   Label dtype: {labels.dtype}\")\n",
    "        print(f\"   Image range: [{images.min():.3f}, {images.max():.3f}]\")\n",
    "        print(f\"   Label range: [{labels.min()}, {labels.max()}]\")\n",
    "        print(f\"   Unique labels in batch: {torch.unique(labels).tolist()}\")\n",
    "        \n",
    "        # Check for ignore index\n",
    "        ignore_count = (labels == CFG['IGNORE_INDEX']).sum().item()\n",
    "        total_pixels = labels.numel()\n",
    "        print(f\"   Ignore pixels: {ignore_count:,} / {total_pixels:,} ({ignore_count/total_pixels*100:.2f}%)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Data loading test failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No dataset available - skipping data loading test\")\n",
    "    print(\"   Please ensure Cityscapes dataset is properly downloaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6132f3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üéØ **LOSS FUNCTION & OPTIMIZER** - TPU Optimized Setup\n",
    "\"\"\"\n",
    "Setup loss function, optimizer, and learning rate scheduler\n",
    "Key configurations:\n",
    "- CrossEntropyLoss with ignore_index=255\n",
    "- SGD optimizer with momentum\n",
    "- Polynomial learning rate decay\n",
    "- TPU-optimized settings\n",
    "\"\"\"\n",
    "\n",
    "# üìâ Loss Function\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=CFG['IGNORE_INDEX'])\n",
    "print(f\"üìâ **Loss Function:** CrossEntropyLoss(ignore_index={CFG['IGNORE_INDEX']})\")\n",
    "\n",
    "# üîß Optimizer Setup  \n",
    "optimizer = optim.SGD(\n",
    "    model.parameters(),\n",
    "    lr=CFG['LEARNING_RATE'],\n",
    "    momentum=CFG['MOMENTUM'],\n",
    "    weight_decay=CFG['WEIGHT_DECAY']\n",
    ")\n",
    "\n",
    "print(f\"üîß **Optimizer:** SGD\")\n",
    "print(f\"   Learning Rate: {CFG['LEARNING_RATE']}\")\n",
    "print(f\"   Momentum: {CFG['MOMENTUM']}\")\n",
    "print(f\"   Weight Decay: {CFG['WEIGHT_DECAY']}\")\n",
    "\n",
    "# üìÖ Learning Rate Scheduler (Polynomial Decay)\n",
    "def poly_lr_scheduler(optimizer, init_lr, iter, max_iter, power):\n",
    "    \"\"\"Polynomial learning rate decay\"\"\"\n",
    "    lr = init_lr * (1 - iter / max_iter) ** power\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    return lr\n",
    "\n",
    "# üî• Mixed Precision Setup\n",
    "if CFG['MIXED_PRECISION']:\n",
    "    if IS_TPU:\n",
    "        # TPU uses bfloat16 automatically\n",
    "        print(\"üî• **Mixed Precision:** bfloat16 (TPU native)\")\n",
    "        scaler = None  # TPU handles this automatically\n",
    "    else:\n",
    "        # GPU uses GradScaler for fp16\n",
    "        from torch.cuda.amp import GradScaler, autocast\n",
    "        scaler = GradScaler()\n",
    "        print(\"üî• **Mixed Precision:** fp16 + GradScaler\")\n",
    "else:\n",
    "    scaler = None\n",
    "    print(\"üî• **Mixed Precision:** Disabled\")\n",
    "\n",
    "# üìä Training State Tracking\n",
    "training_state = {\n",
    "    'iteration': 0,\n",
    "    'best_miou': 0.0,\n",
    "    'train_losses': [],\n",
    "    'val_mious': [],\n",
    "    'learning_rates': []\n",
    "}\n",
    "\n",
    "print(f\"\\nüéØ **Training Configuration:**\")\n",
    "print(f\"   Max Iterations: {CFG['MAX_ITERATIONS']:,}\")\n",
    "print(f\"   Eval Interval: {CFG['EVAL_INTERVAL']:,}\")\n",
    "print(f\"   Save Interval: {CFG['SAVE_INTERVAL']:,}\")\n",
    "print(f\"   Warmup Iterations: {CFG['WARMUP_ITERATIONS']:,}\")\n",
    "print(f\"   Polynomial Power: {CFG['POWER']}\")\n",
    "\n",
    "# üìÅ Create output directory\n",
    "os.makedirs(CFG['OUTPUT_DIR'], exist_ok=True)\n",
    "print(f\"üìÅ Output directory: {CFG['OUTPUT_DIR']}\")\n",
    "\n",
    "# üß™ Test loss computation with proper gradient setup\n",
    "print(\"\\nüß™ Testing loss computation...\")\n",
    "try:\n",
    "    # Create dummy predictions and targets with gradients enabled\n",
    "    dummy_pred = torch.randn(2, CFG['NUM_CLASSES'], 64, 64, requires_grad=True).to(device)\n",
    "    dummy_target = torch.randint(0, CFG['NUM_CLASSES'], (2, 64, 64)).to(device)\n",
    "    \n",
    "    # Add some ignore pixels\n",
    "    dummy_target[0, :10, :10] = CFG['IGNORE_INDEX']\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = criterion(dummy_pred, dummy_target)\n",
    "    \n",
    "    print(f\"‚úÖ **Loss Test Passed:**\")\n",
    "    print(f\"   Dummy prediction shape: {dummy_pred.shape}\")\n",
    "    print(f\"   Dummy target shape: {dummy_target.shape}\")\n",
    "    print(f\"   Loss value: {loss.item():.4f}\")\n",
    "    print(f\"   Requires grad: {dummy_pred.requires_grad}\")\n",
    "    print(f\"   Loss has grad_fn: {loss.grad_fn is not None}\")\n",
    "    \n",
    "    # Test backward pass\n",
    "    loss.backward()\n",
    "    print(f\"   ‚úÖ Backward pass successful\")\n",
    "    print(f\"   Gradient shape: {dummy_pred.grad.shape if dummy_pred.grad is not None else 'None'}\")\n",
    "    \n",
    "    # Clear gradients for clean state\n",
    "    dummy_pred.grad = None\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Loss test failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "# üß™ Test with actual model gradients\n",
    "print(\"\\nüß™ Testing with model gradients...\")\n",
    "try:\n",
    "    # Zero gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Create dummy input and run through model\n",
    "    dummy_input = torch.randn(1, 3, 64, 64).to(device)\n",
    "    dummy_target = torch.randint(0, CFG['NUM_CLASSES'], (1, 64, 64)).to(device)\n",
    "    \n",
    "    # Forward pass through model\n",
    "    model_output = model(dummy_input)\n",
    "    model_loss = criterion(model_output['out'], dummy_target)\n",
    "    \n",
    "    print(f\"‚úÖ **Model Loss Test Passed:**\")\n",
    "    print(f\"   Model output shape: {model_output['out'].shape}\")\n",
    "    print(f\"   Model loss: {model_loss.item():.4f}\")\n",
    "    print(f\"   Loss has grad_fn: {model_loss.grad_fn is not None}\")\n",
    "    \n",
    "    # Test backward pass\n",
    "    model_loss.backward()\n",
    "    print(f\"   ‚úÖ Model backward pass successful\")\n",
    "    \n",
    "    # Check if gradients were computed\n",
    "    total_grad_norm = 0\n",
    "    param_count = 0\n",
    "    for param in model.parameters():\n",
    "        if param.grad is not None:\n",
    "            total_grad_norm += param.grad.data.norm(2).item() ** 2\n",
    "            param_count += 1\n",
    "    \n",
    "    total_grad_norm = total_grad_norm ** 0.5\n",
    "    print(f\"   üìä Gradient norm: {total_grad_norm:.4f} across {param_count} parameters\")\n",
    "    \n",
    "    # Clear gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Model loss test failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a4ffe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä **mIoU EVALUATION METRICS** - Cityscapes Standard\n",
    "\"\"\"\n",
    "Mean Intersection over Union (mIoU) computation for Cityscapes\n",
    "Key features:\n",
    "- Per-class IoU calculation\n",
    "- Proper handling of ignore_index (255)\n",
    "- TPU-optimized computation\n",
    "- Standard Cityscapes evaluation protocol\n",
    "\"\"\"\n",
    "\n",
    "class mIoUCalculator:\n",
    "    \"\"\"\n",
    "    üìä Calculate mean Intersection over Union for semantic segmentation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes, ignore_index=255):\n",
    "        self.num_classes = num_classes\n",
    "        self.ignore_index = ignore_index\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset all statistics\"\"\"\n",
    "        self.confusion_matrix = np.zeros((self.num_classes, self.num_classes))\n",
    "    \n",
    "    def update(self, predictions, targets):\n",
    "        \"\"\"\n",
    "        Update confusion matrix with new predictions and targets\n",
    "        \n",
    "        Args:\n",
    "            predictions: Model predictions (B, H, W) - class indices\n",
    "            targets: Ground truth labels (B, H, W) - class indices\n",
    "        \"\"\"\n",
    "        # Convert to numpy if tensors\n",
    "        if torch.is_tensor(predictions):\n",
    "            predictions = predictions.cpu().numpy()\n",
    "        if torch.is_tensor(targets):\n",
    "            targets = targets.cpu().numpy()\n",
    "        \n",
    "        # Flatten arrays\n",
    "        predictions = predictions.flatten()\n",
    "        targets = targets.flatten()\n",
    "        \n",
    "        # Remove ignore pixels\n",
    "        valid_mask = (targets != self.ignore_index)\n",
    "        predictions = predictions[valid_mask]\n",
    "        targets = targets[valid_mask]\n",
    "        \n",
    "        # Ensure predictions are within valid range\n",
    "        predictions = np.clip(predictions, 0, self.num_classes - 1)\n",
    "        \n",
    "        # Update confusion matrix\n",
    "        for pred, target in zip(predictions, targets):\n",
    "            if 0 <= target < self.num_classes:\n",
    "                self.confusion_matrix[target, pred] += 1\n",
    "    \n",
    "    def compute_iou(self):\n",
    "        \"\"\"\n",
    "        Compute IoU for each class and mean IoU\n",
    "        \n",
    "        Returns:\n",
    "            per_class_iou: IoU for each class\n",
    "            mean_iou: Mean IoU across all classes\n",
    "        \"\"\"\n",
    "        # Calculate IoU for each class\n",
    "        intersection = np.diag(self.confusion_matrix)\n",
    "        union = (\n",
    "            self.confusion_matrix.sum(axis=1) + \n",
    "            self.confusion_matrix.sum(axis=0) - \n",
    "            intersection\n",
    "        )\n",
    "        \n",
    "        # Avoid division by zero\n",
    "        valid_classes = union > 0\n",
    "        per_class_iou = np.zeros(self.num_classes)\n",
    "        per_class_iou[valid_classes] = intersection[valid_classes] / union[valid_classes]\n",
    "        \n",
    "        # Mean IoU (only for classes that appear in ground truth)\n",
    "        mean_iou = per_class_iou[valid_classes].mean() if valid_classes.any() else 0.0\n",
    "        \n",
    "        return per_class_iou, mean_iou\n",
    "    \n",
    "    def get_results(self):\n",
    "        \"\"\"Get detailed results\"\"\"\n",
    "        per_class_iou, mean_iou = self.compute_iou()\n",
    "        \n",
    "        results = {\n",
    "            'mIoU': mean_iou,\n",
    "            'per_class_IoU': per_class_iou,\n",
    "            'confusion_matrix': self.confusion_matrix.copy()\n",
    "        }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def print_results(self, class_names=None):\n",
    "        \"\"\"Print formatted results\"\"\"\n",
    "        per_class_iou, mean_iou = self.compute_iou()\n",
    "        \n",
    "        print(f\"üìä **mIoU Results:**\")\n",
    "        print(f\"   Mean IoU: {mean_iou:.4f} ({mean_iou*100:.2f}%)\")\n",
    "        print(f\"\\nüìã **Per-Class IoU:**\")\n",
    "        \n",
    "        if class_names is None:\n",
    "            class_names = [f\"Class_{i}\" for i in range(self.num_classes)]\n",
    "        \n",
    "        for i, (class_name, iou) in enumerate(zip(class_names, per_class_iou)):\n",
    "            print(f\"   {i:2d}. {class_name:<15}: {iou:.4f} ({iou*100:.2f}%)\")\n",
    "\n",
    "# üß™ Test mIoU calculator\n",
    "print(\"üìä **mIoU CALCULATOR INITIALIZED**\")\n",
    "miou_calculator = mIoUCalculator(\n",
    "    num_classes=CFG['NUM_CLASSES'], \n",
    "    ignore_index=CFG['IGNORE_INDEX']\n",
    ")\n",
    "\n",
    "# Test with dummy data\n",
    "print(\"\\nüß™ Testing mIoU calculator...\")\n",
    "try:\n",
    "    # Create dummy predictions and targets\n",
    "    dummy_predictions = np.random.randint(0, CFG['NUM_CLASSES'], (2, 100, 100))\n",
    "    dummy_targets = np.random.randint(0, CFG['NUM_CLASSES'], (2, 100, 100))\n",
    "    \n",
    "    # Add some ignore pixels\n",
    "    dummy_targets[0, :10, :10] = CFG['IGNORE_INDEX']\n",
    "    \n",
    "    # Update calculator\n",
    "    miou_calculator.update(dummy_predictions, dummy_targets)\n",
    "    \n",
    "    # Compute results\n",
    "    results = miou_calculator.get_results()\n",
    "    \n",
    "    print(f\"‚úÖ **mIoU Test Passed:**\")\n",
    "    print(f\"   Mean IoU: {results['mIoU']:.4f}\")\n",
    "    print(f\"   Confusion matrix shape: {results['confusion_matrix'].shape}\")\n",
    "    print(f\"   Per-class IoU shape: {results['per_class_IoU'].shape}\")\n",
    "    \n",
    "    # Test with class names\n",
    "    miou_calculator.print_results(CITYSCAPES_CLASSES)\n",
    "    \n",
    "    # Reset for actual training\n",
    "    miou_calculator.reset()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå mIoU test failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fb4da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîÑ **EVALUATION FUNCTION** - TPU Optimized Validation\n",
    "\"\"\"\n",
    "Comprehensive evaluation function for Cityscapes validation\n",
    "Key features:\n",
    "- TPU-aware evaluation loop\n",
    "- Memory-efficient processing\n",
    "- mIoU computation with proper aggregation\n",
    "- Progress tracking and logging\n",
    "\"\"\"\n",
    "\n",
    "def evaluate_model(model, val_loader, criterion, miou_calculator, device, is_tpu=False):\n",
    "    \"\"\"\n",
    "    üîç Evaluate model on validation set\n",
    "    \n",
    "    Returns:\n",
    "        eval_results: Dictionary with loss, mIoU, and per-class IoU\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Reset metrics\n",
    "    miou_calculator.reset()\n",
    "    total_loss = 0.0\n",
    "    total_samples = 0\n",
    "    \n",
    "    print(\"üîç Starting validation...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Progress bar\n",
    "        pbar = tqdm(val_loader, desc=\"üß™ Validating\", leave=False) if not is_tpu else val_loader\n",
    "        \n",
    "        for batch_idx, (images, targets) in enumerate(pbar):\n",
    "            # Move to device (already on TPU if using TPU loader)\n",
    "            if not is_tpu:\n",
    "                images = images.to(device)\n",
    "                targets = targets.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            if CFG['MIXED_PRECISION'] and not is_tpu:\n",
    "                # GPU with mixed precision\n",
    "                with autocast():\n",
    "                    outputs = model(images)\n",
    "                    loss = criterion(outputs['out'], targets)\n",
    "            else:\n",
    "                # TPU or regular computation\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs['out'], targets)\n",
    "            \n",
    "            # Accumulate loss\n",
    "            total_loss += loss.item()\n",
    "            total_samples += images.size(0)\n",
    "            \n",
    "            # Get predictions\n",
    "            predictions = torch.argmax(outputs['out'], dim=1)\n",
    "            \n",
    "            # Update mIoU calculator\n",
    "            miou_calculator.update(predictions, targets)\n",
    "            \n",
    "            # Update progress bar\n",
    "            if not is_tpu and hasattr(pbar, 'set_postfix'):\n",
    "                current_loss = total_loss / (batch_idx + 1)\n",
    "                pbar.set_postfix({\n",
    "                    'Loss': f'{current_loss:.4f}',\n",
    "                    'Samples': f'{total_samples:,}'\n",
    "                })\n",
    "            \n",
    "            # Periodic memory cleanup for TPU\n",
    "            if is_tpu and batch_idx % 10 == 0:\n",
    "                xm.mark_step()  # TPU step\n",
    "    \n",
    "    # Compute final metrics\n",
    "    avg_loss = total_loss / len(val_loader)\n",
    "    eval_results = miou_calculator.get_results()\n",
    "    eval_results['loss'] = avg_loss\n",
    "    eval_results['total_samples'] = total_samples\n",
    "    \n",
    "    # TPU synchronization\n",
    "    if is_tpu:\n",
    "        # Reduce metrics across TPU cores\n",
    "        xm.master_print(f\"üîç Validation completed on {total_samples:,} samples\")\n",
    "        # Note: For proper TPU evaluation, we'd need to aggregate metrics across cores\n",
    "        # This is simplified for demonstration\n",
    "    \n",
    "    print(f\"‚úÖ Validation completed:\")\n",
    "    print(f\"   Average Loss: {avg_loss:.4f}\")\n",
    "    print(f\"   Mean IoU: {eval_results['mIoU']:.4f} ({eval_results['mIoU']*100:.2f}%)\")\n",
    "    print(f\"   Total Samples: {total_samples:,}\")\n",
    "    \n",
    "    model.train()  # Set back to training mode\n",
    "    return eval_results\n",
    "\n",
    "# üß™ Test evaluation function (dry run)\n",
    "print(\"üîç **EVALUATION FUNCTION READY**\")\n",
    "print(\"   - TPU-aware validation loop\")\n",
    "print(\"   - Memory-efficient processing\")\n",
    "print(\"   - Comprehensive mIoU computation\")\n",
    "print(\"   - Progress tracking and logging\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58b38c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöÄ **TRAINING LOOP** - TPU Optimized Main Training\n",
    "\"\"\"\n",
    "Complete training loop optimized for TPU v5e-8\n",
    "Key features:\n",
    "- Polynomial learning rate scheduling with warmup\n",
    "- TPU-native mixed precision (bfloat16)\n",
    "- Periodic evaluation and checkpointing\n",
    "- Memory-efficient gradient computation\n",
    "- Comprehensive logging and monitoring\n",
    "\"\"\"\n",
    "\n",
    "def train_model():\n",
    "    \"\"\"\n",
    "    üöÄ Main training function\n",
    "    \"\"\"\n",
    "    print(\"üöÄ **STARTING TRAINING**\")\n",
    "    print(f\"   Device: {device}\")\n",
    "    print(f\"   Max Iterations: {CFG['MAX_ITERATIONS']:,}\")\n",
    "    print(f\"   Effective Batch Size: {(CFG['BATCH_SIZE'] * world_size if IS_TPU else CFG['BATCH_SIZE'])}\")\n",
    "    print(f\"   Learning Rate: {CFG['LEARNING_RATE']}\")\n",
    "    print(f\"   Mixed Precision: {CFG['MIXED_PRECISION']}\")\n",
    "    \n",
    "    # Training state\n",
    "    global training_state\n",
    "    iteration = training_state['iteration']\n",
    "    best_miou = training_state['best_miou']\n",
    "    \n",
    "    # Create progress tracking\n",
    "    if not IS_TPU or xm.is_master_ordinal():\n",
    "        pbar = tqdm(total=CFG['MAX_ITERATIONS'], initial=iteration, desc=\"üöÄ Training\")\n",
    "    \n",
    "    # Training loop\n",
    "    model.train()\n",
    "    \n",
    "    while iteration < CFG['MAX_ITERATIONS']:\n",
    "        \n",
    "        # Set epoch for distributed sampler\n",
    "        if IS_TPU:\n",
    "            epoch = iteration // len(train_loader)\n",
    "            train_loader._loader.sampler.set_epoch(epoch)\n",
    "        \n",
    "        for batch_idx, (images, targets) in enumerate(train_loader):\n",
    "            \n",
    "            # Check iteration limit\n",
    "            if iteration >= CFG['MAX_ITERATIONS']:\n",
    "                break\n",
    "            \n",
    "            # Move to device (already on TPU if using TPU loader)\n",
    "            if not IS_TPU:\n",
    "                images = images.to(device)\n",
    "                targets = targets.to(device)\n",
    "            \n",
    "            # Learning rate scheduling with warmup\n",
    "            if iteration < CFG['WARMUP_ITERATIONS']:\n",
    "                # Linear warmup\n",
    "                lr = CFG['LEARNING_RATE'] * (iteration / CFG['WARMUP_ITERATIONS'])\n",
    "            else:\n",
    "                # Polynomial decay\n",
    "                lr = poly_lr_scheduler(\n",
    "                    optimizer, \n",
    "                    CFG['LEARNING_RATE'],\n",
    "                    iteration - CFG['WARMUP_ITERATIONS'],\n",
    "                    CFG['MAX_ITERATIONS'] - CFG['WARMUP_ITERATIONS'],\n",
    "                    CFG['POWER']\n",
    "                )\n",
    "            \n",
    "            # Set learning rate\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "            \n",
    "            # Zero gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass with mixed precision\n",
    "            if CFG['MIXED_PRECISION'] and not IS_TPU:\n",
    "                # GPU mixed precision\n",
    "                with autocast():\n",
    "                    outputs = model(images)\n",
    "                    main_loss = criterion(outputs['out'], targets)\n",
    "                    \n",
    "                    # Auxiliary loss (if available)\n",
    "                    if 'aux' in outputs and outputs['aux'] is not None:\n",
    "                        aux_loss = criterion(outputs['aux'], targets)\n",
    "                        loss = main_loss + 0.4 * aux_loss  # Standard weight for aux loss\n",
    "                    else:\n",
    "                        loss = main_loss\n",
    "                \n",
    "                # Backward pass with scaling\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                \n",
    "            else:\n",
    "                # TPU or regular computation\n",
    "                outputs = model(images)\n",
    "                main_loss = criterion(outputs['out'], targets)\n",
    "                \n",
    "                # Auxiliary loss (if available)\n",
    "                if 'aux' in outputs and outputs['aux'] is not None:\n",
    "                    aux_loss = criterion(outputs['aux'], targets)\n",
    "                    loss = main_loss + 0.4 * aux_loss\n",
    "                else:\n",
    "                    loss = main_loss\n",
    "                \n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            # TPU step marking\n",
    "            if IS_TPU:\n",
    "                xm.mark_step()\n",
    "            \n",
    "            # Record training state\n",
    "            training_state['train_losses'].append(loss.item())\n",
    "            training_state['learning_rates'].append(lr)\n",
    "            iteration += 1\n",
    "            training_state['iteration'] = iteration\n",
    "            \n",
    "            # Update progress bar\n",
    "            if not IS_TPU or xm.is_master_ordinal():\n",
    "                pbar.set_postfix({\n",
    "                    'Loss': f'{loss.item():.4f}',\n",
    "                    'LR': f'{lr:.6f}',\n",
    "                    'Iter': f'{iteration}/{CFG[\"MAX_ITERATIONS\"]}'\n",
    "                })\n",
    "                pbar.update(1)\n",
    "            \n",
    "            # Evaluation\n",
    "            if iteration % CFG['EVAL_INTERVAL'] == 0:\n",
    "                print(f\"\\nüîç **Evaluation at iteration {iteration:,}**\")\n",
    "                \n",
    "                # Run evaluation\n",
    "                eval_results = evaluate_model(\n",
    "                    model, val_loader, criterion, \n",
    "                    miou_calculator, device, IS_TPU\n",
    "                )\n",
    "                \n",
    "                current_miou = eval_results['mIoU']\n",
    "                training_state['val_mious'].append(current_miou)\n",
    "                \n",
    "                # Check for best model\n",
    "                if current_miou > best_miou:\n",
    "                    best_miou = current_miou\n",
    "                    training_state['best_miou'] = best_miou\n",
    "                    \n",
    "                    # Save best model\n",
    "                    if not IS_TPU or xm.is_master_ordinal():\n",
    "                        model_path = os.path.join(CFG['OUTPUT_DIR'], 'best_model.pth')\n",
    "                        torch.save({\n",
    "                            'iteration': iteration,\n",
    "                            'model_state_dict': model.state_dict(),\n",
    "                            'optimizer_state_dict': optimizer.state_dict(),\n",
    "                            'best_miou': best_miou,\n",
    "                            'config': CFG\n",
    "                        }, model_path)\n",
    "                        print(f\"üíæ Best model saved: {model_path} (mIoU: {best_miou:.4f})\")\n",
    "                \n",
    "                print(f\"   Current mIoU: {current_miou:.4f}\")\n",
    "                print(f\"   Best mIoU: {best_miou:.4f}\")\n",
    "                \n",
    "                # Print per-class results\n",
    "                miou_calculator.print_results(CITYSCAPES_CLASSES)\n",
    "                \n",
    "                model.train()  # Set back to training mode\n",
    "            \n",
    "            # Save checkpoint\n",
    "            if iteration % CFG['SAVE_INTERVAL'] == 0:\n",
    "                if not IS_TPU or xm.is_master_ordinal():\n",
    "                    checkpoint_path = os.path.join(CFG['OUTPUT_DIR'], f'checkpoint_{iteration}.pth')\n",
    "                    torch.save({\n",
    "                        'iteration': iteration,\n",
    "                        'model_state_dict': model.state_dict(),\n",
    "                        'optimizer_state_dict': optimizer.state_dict(),\n",
    "                        'training_state': training_state,\n",
    "                        'config': CFG\n",
    "                    }, checkpoint_path)\n",
    "                    print(f\"\\nüíæ Checkpoint saved: {checkpoint_path}\")\n",
    "            \n",
    "            # Memory cleanup\n",
    "            del outputs, loss\n",
    "            if IS_TPU and iteration % 50 == 0:\n",
    "                xm.mark_step()  # Additional TPU synchronization\n",
    "    \n",
    "    # Close progress bar\n",
    "    if not IS_TPU or xm.is_master_ordinal():\n",
    "        pbar.close()\n",
    "    \n",
    "    print(f\"\\n‚úÖ **Training completed!**\")\n",
    "    print(f\"   Total iterations: {iteration:,}\")\n",
    "    print(f\"   Best mIoU: {best_miou:.4f} ({best_miou*100:.2f}%)\")\n",
    "    \n",
    "    return training_state\n",
    "\n",
    "print(\"üöÄ **TRAINING LOOP READY**\")\n",
    "print(\"   - TPU-optimized training with bfloat16\")\n",
    "print(\"   - Polynomial LR scheduling with warmup\")\n",
    "print(\"   - Periodic evaluation and checkpointing\")\n",
    "print(\"   - Memory-efficient gradient computation\")\n",
    "print(\"   - Comprehensive progress tracking\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69b99a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üé¨ **START TRAINING** - Execute Training Loop\n",
    "\"\"\"\n",
    "Launch the complete training process\n",
    "This will run for 40,000 iterations on TPU v5e-8\n",
    "\"\"\"\n",
    "\n",
    "# üöÄ Start training\n",
    "print(\"üé¨ **LAUNCHING CITYSCAPES TRAINING**\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    # Run training\n",
    "    final_state = train_model()\n",
    "    \n",
    "    print(\"\\nüéâ **TRAINING COMPLETED SUCCESSFULLY!**\")\n",
    "    print(f\"   Final Best mIoU: {final_state['best_miou']:.4f}\")\n",
    "    print(f\"   Total Iterations: {final_state['iteration']:,}\")\n",
    "    print(f\"   Training Losses: {len(final_state['train_losses']):,} recorded\")\n",
    "    print(f\"   Validation mIoUs: {len(final_state['val_mious'])} recorded\")\n",
    "    \n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n‚ö†Ô∏è Training interrupted by user\")\n",
    "    print(f\"   Current iteration: {training_state['iteration']:,}\")\n",
    "    print(f\"   Current best mIoU: {training_state['best_miou']:.4f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Training failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    \n",
    "    # Save emergency checkpoint if possible\n",
    "    try:\n",
    "        emergency_path = os.path.join(CFG['OUTPUT_DIR'], 'emergency_checkpoint.pth')\n",
    "        torch.save({\n",
    "            'iteration': training_state['iteration'],\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'training_state': training_state,\n",
    "            'config': CFG,\n",
    "            'error': str(e)\n",
    "        }, emergency_path)\n",
    "        print(f\"üíæ Emergency checkpoint saved: {emergency_path}\")\n",
    "    except:\n",
    "        print(\"‚ùå Could not save emergency checkpoint\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2acc8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä **TRAINING VISUALIZATION & ANALYSIS** - Results Dashboard\n",
    "\"\"\"\n",
    "Visualize training progress and analyze results\n",
    "Key features:\n",
    "- Training loss curves\n",
    "- Validation mIoU progression\n",
    "- Learning rate schedule\n",
    "- Per-class IoU breakdown\n",
    "- Model performance analysis\n",
    "\"\"\"\n",
    "\n",
    "def plot_training_results(training_state):\n",
    "    \"\"\"\n",
    "    üìà Plot comprehensive training results\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('üèôÔ∏è DeepLabv3+ Cityscapes Training Results', fontsize=16, y=0.98)\n",
    "    \n",
    "    # 1. Training Loss\n",
    "    if training_state['train_losses']:\n",
    "        axes[0, 0].plot(training_state['train_losses'], 'b-', alpha=0.7, linewidth=1)\n",
    "        axes[0, 0].set_title('üìâ Training Loss')\n",
    "        axes[0, 0].set_xlabel('Iteration')\n",
    "        axes[0, 0].set_ylabel('Loss')\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add smoothed line\n",
    "        if len(training_state['train_losses']) > 100:\n",
    "            window = len(training_state['train_losses']) // 50\n",
    "            smooth_loss = np.convolve(training_state['train_losses'], \n",
    "                                    np.ones(window)/window, mode='valid')\n",
    "            smooth_x = np.arange(window//2, len(training_state['train_losses']) - window//2 + 1)\n",
    "            axes[0, 0].plot(smooth_x, smooth_loss, 'r-', linewidth=2, label='Smoothed')\n",
    "            axes[0, 0].legend()\n",
    "    \n",
    "    # 2. Validation mIoU\n",
    "    if training_state['val_mious']:\n",
    "        eval_iterations = [CFG['EVAL_INTERVAL'] * (i+1) for i in range(len(training_state['val_mious']))]\n",
    "        axes[0, 1].plot(eval_iterations, training_state['val_mious'], 'g-o', linewidth=2, markersize=6)\n",
    "        axes[0, 1].set_title('üìä Validation mIoU')\n",
    "        axes[0, 1].set_xlabel('Iteration')\n",
    "        axes[0, 1].set_ylabel('mIoU')\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        axes[0, 1].set_ylim(0, 1)\n",
    "        \n",
    "        # Highlight best score\n",
    "        best_idx = np.argmax(training_state['val_mious'])\n",
    "        best_iter = eval_iterations[best_idx]\n",
    "        best_miou = training_state['val_mious'][best_idx]\n",
    "        axes[0, 1].scatter([best_iter], [best_miou], color='red', s=100, zorder=5)\n",
    "        axes[0, 1].annotate(f'Best: {best_miou:.4f}', \n",
    "                          xy=(best_iter, best_miou), \n",
    "                          xytext=(10, 10), textcoords='offset points',\n",
    "                          bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.7))\n",
    "    \n",
    "    # 3. Learning Rate Schedule\n",
    "    if training_state['learning_rates']:\n",
    "        axes[1, 0].plot(training_state['learning_rates'], 'purple', linewidth=2)\n",
    "        axes[1, 0].set_title('üìÖ Learning Rate Schedule')\n",
    "        axes[1, 0].set_xlabel('Iteration')\n",
    "        axes[1, 0].set_ylabel('Learning Rate')\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        axes[1, 0].set_yscale('log')\n",
    "    \n",
    "    # 4. Training Summary\n",
    "    axes[1, 1].axis('off')\n",
    "    summary_text = f\\\"\\\"\\\"\\nüéØ Training Summary:\\n\\n‚Ä¢ Total Iterations: {training_state['iteration']:,}\\n‚Ä¢ Best mIoU: {training_state['best_miou']:.4f} ({training_state['best_miou']*100:.2f}%)\\n‚Ä¢ Final Loss: {training_state['train_losses'][-1]:.4f if training_state['train_losses'] else 'N/A'}\\n‚Ä¢ Dataset: Cityscapes (19 classes)\\n‚Ä¢ Resolution: {CFG['CROP_SIZE']}√ó{CFG['CROP_SIZE']}\\n‚Ä¢ Batch Size: {CFG['BATCH_SIZE']} per core\\n‚Ä¢ TPU Cores: {CFG['TPU_CORES']}\\n‚Ä¢ Mixed Precision: {CFG['MIXED_PRECISION']}\\n‚Ä¢ Backbone: {CFG['BACKBONE']}\\n\\\"\\\"\\\"\\n    axes[1, 1].text(0.1, 0.9, summary_text, transform=axes[1, 1].transAxes, \\n                    fontsize=11, verticalalignment='top', \\n                    bbox=dict(boxstyle='round,pad=0.5', facecolor='lightblue', alpha=0.8))\\n    \\n    plt.tight_layout()\\n    plt.show()\\n    \\n    # Save plot\\n    plot_path = os.path.join(CFG['OUTPUT_DIR'], 'training_results.png')\\n    fig.savefig(plot_path, dpi=150, bbox_inches='tight')\\n    print(f\\\"üìä Training plot saved: {plot_path}\\\")\\n\\ndef analyze_final_results(training_state):\\n    \\\"\\\"\\\"üìã Analyze and summarize final training results\\\"\\\"\\\"\\n    print(\\\"\\\\nüìã **FINAL TRAINING ANALYSIS**\\\")\\n    print(\\\"=\\\" * 50)\\n    \\n    # Basic stats\\n    print(f\\\"üéØ **Training Completed:**\\\")\\n    print(f\\\"   Total Iterations: {training_state['iteration']:,}\\\")\\n    print(f\\\"   Best mIoU: {training_state['best_miou']:.4f} ({training_state['best_miou']*100:.2f}%)\\\")\\n    \\n    if training_state['train_losses']:\\n        final_loss = training_state['train_losses'][-1]\\n        avg_loss = np.mean(training_state['train_losses'][-1000:])  # Last 1000 iterations\\n        print(f\\\"   Final Loss: {final_loss:.4f}\\\")\\n        print(f\\\"   Average Loss (last 1000): {avg_loss:.4f}\\\")\\n    \\n    if training_state['val_mious']:\\n        print(f\\\"   Evaluations: {len(training_state['val_mious'])}\\\")\\n        print(f\\\"   mIoU Improvement: {training_state['val_mious'][-1] - training_state['val_mious'][0]:.4f}\\\")\\n    \\n    # Model files\\n    print(f\\\"\\\\nüìÅ **Output Files:**\\\")\\n    output_files = os.listdir(CFG['OUTPUT_DIR'])\\n    for file in sorted(output_files):\\n        file_path = os.path.join(CFG['OUTPUT_DIR'], file)\\n        size_mb = os.path.getsize(file_path) / (1024 * 1024)\\n        print(f\\\"   {file}: {size_mb:.1f} MB\\\")\\n    \\n    print(\\\"=\\\" * 50)\\n\\n# üìä Check if training has results to visualize\\nif training_state['iteration'] > 0:\\n    print(\\\"üìä **VISUALIZING TRAINING RESULTS**\\\")\\n    plot_training_results(training_state)\\n    analyze_final_results(training_state)\\nelse:\\n    print(\\\"üìä **NO TRAINING DATA TO VISUALIZE**\\\")\\n    print(\\\"   Run the training cells above first!\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259aab23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üñºÔ∏è **INFERENCE & VISUALIZATION** - Model Predictions\n",
    "\"\"\"\n",
    "Inference pipeline for trained model\n",
    "Key features:\n",
    "- Load best model checkpoint\n",
    "- Inference on validation samples\n",
    "- Visualization of predictions vs ground truth\n",
    "- Cityscapes color mapping for beautiful results\n",
    "\"\"\"\n",
    "\n",
    "# üé® Cityscapes Color Palette (for visualization)\n",
    "CITYSCAPES_COLORS = [\n",
    "    [128, 64, 128],   # road\n",
    "    [244, 35, 232],   # sidewalk  \n",
    "    [70, 70, 70],     # building\n",
    "    [102, 102, 156],  # wall\n",
    "    [190, 153, 153],  # fence\n",
    "    [153, 153, 153],  # pole\n",
    "    [250, 170, 30],   # traffic light\n",
    "    [220, 220, 0],    # traffic sign\n",
    "    [107, 142, 35],   # vegetation\n",
    "    [152, 251, 152],  # terrain\n",
    "    [70, 130, 180],   # sky\n",
    "    [220, 20, 60],    # person\n",
    "    [255, 0, 0],      # rider\n",
    "    [0, 0, 142],      # car\n",
    "    [0, 0, 70],       # truck\n",
    "    [0, 60, 100],     # bus\n",
    "    [0, 80, 100],     # train\n",
    "    [0, 0, 230],      # motorcycle\n",
    "    [119, 11, 32]     # bicycle\n",
    "]\n",
    "\n",
    "def load_best_model():\n",
    "    \"\"\"üì• Load the best trained model\"\"\"\n",
    "    model_path = os.path.join(CFG['OUTPUT_DIR'], 'best_model.pth')\n",
    "    \n",
    "    if os.path.exists(model_path):\n",
    "        print(f\"üì• Loading best model: {model_path}\")\n",
    "        checkpoint = torch.load(model_path, map_location=device)\n",
    "        \n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        best_miou = checkpoint.get('best_miou', 0.0)\n",
    "        iteration = checkpoint.get('iteration', 0)\n",
    "        \n",
    "        print(f\"‚úÖ Model loaded successfully:\")\n",
    "        print(f\"   Iteration: {iteration:,}\")\n",
    "        print(f\"   Best mIoU: {best_miou:.4f}\")\n",
    "        \n",
    "        model.eval()\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"‚ùå Model not found: {model_path}\")\n",
    "        print(\"   Train the model first!\")\n",
    "        return False\n",
    "\n",
    "def colorize_prediction(prediction):\n",
    "    \"\"\"üé® Convert prediction to colored image\"\"\"\n",
    "    h, w = prediction.shape\n",
    "    colored = np.zeros((h, w, 3), dtype=np.uint8)\n",
    "    \n",
    "    for class_id, color in enumerate(CITYSCAPES_COLORS):\n",
    "        mask = (prediction == class_id)\n",
    "        colored[mask] = color\n",
    "    \n",
    "    return colored\n",
    "\n",
    "def inference_on_samples(num_samples=4):\n",
    "    \"\"\"üîç Run inference on validation samples\"\"\"\n",
    "    if not load_best_model():\n",
    "        return\n",
    "    \n",
    "    print(f\"üîç Running inference on {num_samples} validation samples...\")\n",
    "    \n",
    "    # Get samples from validation set\n",
    "    val_iter = iter(val_loader)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for sample_idx in range(min(num_samples, len(val_loader))):\n",
    "            try:\n",
    "                images, targets = next(val_iter)\n",
    "                \n",
    "                # Take first image from batch\n",
    "                image = images[0:1]  # Keep batch dimension\n",
    "                target = targets[0].cpu().numpy()\n",
    "                \n",
    "                # Move to device\n",
    "                if not IS_TPU:\n",
    "                    image = image.to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(image)\n",
    "                prediction = torch.argmax(outputs['out'], dim=1)[0].cpu().numpy()\n",
    "                \n",
    "                # Convert to visualization format\n",
    "                original_img = images[0].cpu().numpy().transpose(1, 2, 0)\n",
    "                # Denormalize image\n",
    "                mean = np.array(CFG['MEAN'])\n",
    "                std = np.array(CFG['STD'])\n",
    "                original_img = (original_img * std + mean) * 255\n",
    "                original_img = np.clip(original_img, 0, 255).astype(np.uint8)\n",
    "                \n",
    "                # Colorize masks\n",
    "                target_colored = colorize_prediction(target)\n",
    "                pred_colored = colorize_prediction(prediction)\n",
    "                \n",
    "                # Create visualization\n",
    "                fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "                fig.suptitle(f'üèôÔ∏è Sample {sample_idx + 1} - Cityscapes Inference', fontsize=14)\n",
    "                \n",
    "                # Original image\n",
    "                axes[0].imshow(original_img)\n",
    "                axes[0].set_title('üì∑ Original Image')\n",
    "                axes[0].axis('off')\n",
    "                \n",
    "                # Ground truth\n",
    "                axes[1].imshow(target_colored)\n",
    "                axes[1].set_title('üéØ Ground Truth')\n",
    "                axes[1].axis('off')\n",
    "                \n",
    "                # Prediction\n",
    "                axes[2].imshow(pred_colored)\n",
    "                axes[2].set_title('ü§ñ Prediction')  \n",
    "                axes[2].axis('off')\n",
    "                \n",
    "                # Overlay\n",
    "                alpha = 0.6\n",
    "                overlay = (alpha * original_img + (1-alpha) * pred_colored).astype(np.uint8)\n",
    "                axes[3].imshow(overlay)\n",
    "                axes[3].set_title('üé® Overlay')\n",
    "                axes[3].axis('off')\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "                \n",
    "                # Compute sample mIoU\n",
    "                sample_miou = mIoUCalculator(CFG['NUM_CLASSES'], CFG['IGNORE_INDEX'])\n",
    "                sample_miou.update(prediction, target)\n",
    "                results = sample_miou.get_results()\n",
    "                \n",
    "                print(f\"   Sample {sample_idx + 1} mIoU: {results['mIoU']:.4f}\")\n",
    "                \n",
    "                # Save inference result\n",
    "                inference_path = os.path.join(CFG['OUTPUT_DIR'], f'inference_sample_{sample_idx+1}.png')\n",
    "                fig.savefig(inference_path, dpi=150, bbox_inches='tight')\n",
    "                print(f\"   üíæ Saved: {inference_path}\")\n",
    "                \n",
    "            except StopIteration:\n",
    "                print(f\"   ‚ö†Ô∏è Only {sample_idx} samples available\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ùå Error processing sample {sample_idx + 1}: {e}\")\n",
    "\n",
    "# üñºÔ∏è Create class legend\n",
    "def create_class_legend():\n",
    "    \"\"\"üé® Create color legend for Cityscapes classes\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    # Create color patches\n",
    "    colors_normalized = [[c/255.0 for c in color] for color in CITYSCAPES_COLORS]\n",
    "    \n",
    "    # Display as grid\n",
    "    n_cols = 4\n",
    "    n_rows = (len(CITYSCAPES_CLASSES) + n_cols - 1) // n_cols\n",
    "    \n",
    "    for i, (class_name, color) in enumerate(zip(CITYSCAPES_CLASSES, colors_normalized)):\n",
    "        row = i // n_cols\n",
    "        col = i % n_cols\n",
    "        \n",
    "        # Create rectangle\n",
    "        rect = plt.Rectangle((col, n_rows - row - 1), 0.8, 0.8, \n",
    "                           facecolor=color, edgecolor='black', linewidth=1)\n",
    "        ax.add_patch(rect)\n",
    "        \n",
    "        # Add text\n",
    "        ax.text(col + 0.4, n_rows - row - 0.5, f\"{i}: {class_name}\", \n",
    "               ha='center', va='center', fontsize=10, weight='bold')\n",
    "    \n",
    "    ax.set_xlim(0, n_cols)\n",
    "    ax.set_ylim(0, n_rows)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.axis('off')\n",
    "    ax.set_title('üèôÔ∏è Cityscapes Classes Color Legend', fontsize=16, pad=20)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Save legend\n",
    "    legend_path = os.path.join(CFG['OUTPUT_DIR'], 'cityscapes_legend.png')\n",
    "    fig.savefig(legend_path, dpi=150, bbox_inches='tight')\n",
    "    print(f\"üé® Class legend saved: {legend_path}\")\n",
    "\n",
    "print(\"üñºÔ∏è **INFERENCE PIPELINE READY**\")\n",
    "print(\"   - Load best model checkpoint\")\n",
    "print(\"   - Inference on validation samples\") \n",
    "print(\"   - Beautiful Cityscapes color visualization\")\n",
    "print(\"   - Per-sample mIoU computation\")\n",
    "\n",
    "# Show class legend\n",
    "create_class_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227109b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöÄ **RUN INFERENCE** - Generate Beautiful Predictions\n",
    "\"\"\"\n",
    "Execute inference on validation samples to see model results\n",
    "\"\"\"\n",
    "\n",
    "# üîç Run inference on samples\n",
    "print(\"üîç **STARTING INFERENCE**\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "try:\n",
    "    # Run inference on 4 validation samples\n",
    "    inference_on_samples(num_samples=4)\n",
    "    \n",
    "    print(\"\\n‚úÖ **INFERENCE COMPLETED SUCCESSFULLY!**\")\n",
    "    print(\"   Check the visualizations above and saved PNG files\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Inference failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(\"=\" * 40)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
