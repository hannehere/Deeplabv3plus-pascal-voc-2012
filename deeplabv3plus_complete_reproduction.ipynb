{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35f8fdab",
   "metadata": {},
   "source": [
    "# DeepLabv3+ PASCAL VOC 2012 Complete Reproduction\n",
    "\n",
    "ƒê√¢y l√† notebook ho√†n ch·ªânh ƒë·ªÉ t√°i t·∫°o hi·ªáu su·∫•t c·ªßa m√¥ h√¨nh DeepLabv3+ tr√™n b·ªô d·ªØ li·ªáu PASCAL VOC 2012 semantic segmentation, tu√¢n th·ªß ch√≠nh x√°c c√°c th√¥ng s·ªë k·ªπ thu·∫≠t t·ª´ paper g·ªëc.\n",
    "\n",
    "## T√≠nh nƒÉng ch√≠nh:\n",
    "- **M√¥ h√¨nh**: DeepLabv3+ v·ªõi backbone ResNet-101 (torchvision implementation)\n",
    "- **D·ªØ li·ªáu**: PASCAL VOC 2012 (21 classes bao g·ªìm background)\n",
    "- **Hu·∫•n luy·ªán**: SGD optimizer v·ªõi polynomial learning rate scheduling\n",
    "- **Augmentation**: Pipeline Albumentations ph√π h·ª£p v·ªõi specifications c·ªßa paper\n",
    "- **ƒê√°nh gi√°**: mIoU (mean Intersection over Union) metric\n",
    "- **Inference**: Visualization v√† testing tr√™n test images\n",
    "\n",
    "## Tham kh·∫£o Paper:\n",
    "*Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation* (Chen et al., 2018)\n",
    "\n",
    "## Notebook Structure:\n",
    "1. **Environment Setup** - Imports v√† configuration\n",
    "2. **Dataset Download** - T·ª± ƒë·ªông download PASCAL VOC 2012\n",
    "3. **Data Pipeline** - Augmentation v√† DataLoaders\n",
    "4. **Model Setup** - DeepLabv3+ architecture\n",
    "5. **Training** - Complete training loop v·ªõi monitoring\n",
    "6. **Visualization** - Training curves v√† predictions\n",
    "7. **Testing** - Model inference v√† evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a39979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== ENVIRONMENT SETUP AND IMPORTS =====\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "from torchvision.models.segmentation import deeplabv3_resnet101\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "print(\"üöÄ Environment Setup Complete!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Torchvision version: {torchvision.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd471093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== CONFIGURATION AND DEVICE SETUP =====\n",
    "\n",
    "# Comprehensive configuration dictionary - SINGLE SOURCE OF TRUTH\n",
    "CFG = {\n",
    "    # === DEVICE CONFIGURATION ===\n",
    "    'DEVICE': torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    \n",
    "    # === DATA PATHS ===\n",
    "    'DATA_ROOT': '/kaggle/input/pascal-voc-2012/VOCdevkit/VOC2012/',\n",
    "    'IMAGE_SET_FILE_TRAIN': '/kaggle/input/pascal-voc-2012/VOCdevkit/VOC2012/ImageSets/Segmentation/train.txt',\n",
    "    'IMAGE_SET_FILE_VAL': '/kaggle/input/pascal-voc-2012/VOCdevkit/VOC2012/ImageSets/Segmentation/val.txt',\n",
    "    \n",
    "    # === MODEL CONFIGURATION ===\n",
    "    'NUM_CLASSES': 21,\n",
    "    'IGNORE_INDEX': 255,\n",
    "    'BACKBONE': 'resnet101',\n",
    "    'OUTPUT_STRIDE': 16,  # Standard for DeepLabv3+\n",
    "    \n",
    "    # === TRAINING HYPERPARAMETERS (OPTIMIZED FOR T4 GPU) ===\n",
    "    'BATCH_SIZE': 4,  # Reduced from 8 to 4 for T4 GPU\n",
    "    'CROP_SIZE': 384,  # Reduced from 513 to 384 for memory efficiency\n",
    "    'BASE_LR': 0.005,  # Adjusted LR for smaller batch size\n",
    "    'MOMENTUM': 0.9,\n",
    "    'WEIGHT_DECAY': 0.0001,\n",
    "    'MAX_ITERATIONS': 20000,  # Reduced iterations for faster training\n",
    "    'POLY_POWER': 0.9,\n",
    "    \n",
    "    # === AUGMENTATION PARAMETERS ===\n",
    "    'SCALE_MIN': 0.5,\n",
    "    'SCALE_MAX': 2.0,\n",
    "    'FLIP_PROB': 0.5,\n",
    "    \n",
    "    # === MEMORY OPTIMIZATION ===\n",
    "    'GRADIENT_ACCUMULATION_STEPS': 2,  # Simulate larger batch size\n",
    "    'MIXED_PRECISION': True,  # Enable mixed precision training\n",
    "    'NUM_WORKERS': 2,  # Reduced workers for memory\n",
    "    \n",
    "    # === PATHS ===\n",
    "    'MODEL_SAVE_PATH': '/kaggle/working/best_deeplabv3plus_model.pth',\n",
    "    'RESULTS_PATH': '/kaggle/working/',\n",
    "    \n",
    "    # === PASCAL VOC CLASS NAMES ===\n",
    "    'VOC_CLASSES': [\n",
    "        'background', 'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus',\n",
    "        'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse',\n",
    "        'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# === GPU MEMORY OPTIMIZATION ===\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "def clear_gpu_memory():\n",
    "    \"\"\"Clear GPU memory cache\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "def setup_memory_optimization():\n",
    "    \"\"\"Setup memory optimization for training\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        # Set memory allocation strategy\n",
    "        torch.cuda.set_per_process_memory_fraction(0.9)  # Use 90% of GPU memory\n",
    "        \n",
    "        # Enable memory efficiency\n",
    "        torch.backends.cudnn.benchmark = True  # Optimize for fixed input size\n",
    "        torch.backends.cudnn.deterministic = False  # Allow non-deterministic for speed\n",
    "        \n",
    "        # Set expandable segments for better memory management\n",
    "        import os\n",
    "        os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "# Apply memory optimizations\n",
    "setup_memory_optimization()\n",
    "clear_gpu_memory()\n",
    "\n",
    "# === DEVICE INFORMATION ===\n",
    "print(\"üîß DEVICE AND CONFIGURATION SETUP\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Device: {CFG['DEVICE']}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Count: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        memory_gb = torch.cuda.get_device_properties(i).total_memory / 1024**3\n",
    "        print(f\"   ‚îî‚îÄ Memory: {memory_gb:.1f} GB\")\n",
    "    \n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    \n",
    "    # Show current memory usage\n",
    "    allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "    reserved = torch.cuda.memory_reserved() / 1024**3\n",
    "    print(f\"Current GPU Memory Usage:\")\n",
    "    print(f\"   ‚îî‚îÄ Allocated: {allocated:.2f} GB\")\n",
    "    print(f\"   ‚îî‚îÄ Reserved: {reserved:.2f} GB\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  CUDA not available, using CPU\")\n",
    "\n",
    "print(f\"\\nüìã TRAINING CONFIGURATION (OPTIMIZED FOR T4):\")\n",
    "print(f\"  - Model: DeepLabv3+ with {CFG['BACKBONE']} backbone\")\n",
    "print(f\"  - Classes: {CFG['NUM_CLASSES']} (PASCAL VOC)\")\n",
    "print(f\"  - Batch Size: {CFG['BATCH_SIZE']} (optimized for T4)\")\n",
    "print(f\"  - Input Size: {CFG['CROP_SIZE']}x{CFG['CROP_SIZE']} (memory efficient)\")\n",
    "print(f\"  - Max Iterations: {CFG['MAX_ITERATIONS']:,}\")\n",
    "print(f\"  - Base Learning Rate: {CFG['BASE_LR']}\")\n",
    "print(f\"  - Gradient Accumulation: {CFG['GRADIENT_ACCUMULATION_STEPS']} steps\")\n",
    "print(f\"  - Mixed Precision: {CFG['MIXED_PRECISION']}\")\n",
    "print(f\"  - Effective Batch Size: {CFG['BATCH_SIZE'] * CFG['GRADIENT_ACCUMULATION_STEPS']}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcf73c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== DATASET DOWNLOAD AND VERIFICATION =====\n",
    "\n",
    "import kagglehub\n",
    "\n",
    "print(\"üì¶ PASCAL VOC 2012 DATASET DOWNLOAD\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Downloading PASCAL VOC 2012 dataset...\")\n",
    "print(\"‚è≥ This may take a few minutes depending on your internet connection...\")\n",
    "\n",
    "try:\n",
    "    # Download latest version of PASCAL VOC 2012 dataset\n",
    "    dataset_path = kagglehub.dataset_download(\"gopalbhattrai/pascal-voc-2012-dataset\")\n",
    "    \n",
    "    print(f\"‚úÖ Dataset downloaded successfully!\")\n",
    "    print(f\"üìÅ Dataset path: {dataset_path}\")\n",
    "    \n",
    "    # Explore the actual directory structure\n",
    "    print(f\"\\nüîç Exploring dataset structure...\")\n",
    "    def explore_directory(path, max_depth=3, current_depth=0):\n",
    "        items = []\n",
    "        if current_depth >= max_depth:\n",
    "            return items\n",
    "        \n",
    "        try:\n",
    "            for item in os.listdir(path):\n",
    "                item_path = os.path.join(path, item)\n",
    "                if os.path.isdir(item_path):\n",
    "                    items.append(f\"{'  ' * current_depth}üìÅ {item}/\")\n",
    "                    items.extend(explore_directory(item_path, max_depth, current_depth + 1))\n",
    "                else:\n",
    "                    items.append(f\"{'  ' * current_depth}üìÑ {item}\")\n",
    "        except PermissionError:\n",
    "            items.append(f\"{'  ' * current_depth}‚ùå Permission denied\")\n",
    "        \n",
    "        return items\n",
    "    \n",
    "    structure = explore_directory(dataset_path, max_depth=4)\n",
    "    for item in structure[:20]:  # Show first 20 items\n",
    "        print(item)\n",
    "    \n",
    "    if len(structure) > 20:\n",
    "        print(f\"... and {len(structure) - 20} more items\")\n",
    "    \n",
    "    # Try different possible structures\n",
    "    possible_roots = [\n",
    "        dataset_path,\n",
    "        os.path.join(dataset_path, 'VOCdevkit', 'VOC2012'),\n",
    "        os.path.join(dataset_path, 'VOC2012'),\n",
    "        os.path.join(dataset_path, 'pascal-voc-2012-dataset'),\n",
    "        os.path.join(dataset_path, 'pascal-voc-2012-dataset', 'VOCdevkit', 'VOC2012')\n",
    "    ]\n",
    "    \n",
    "    dataset_root = None\n",
    "    for possible_root in possible_roots:\n",
    "        if os.path.exists(possible_root):\n",
    "            # Check if this contains the expected VOC structure\n",
    "            expected_subdirs = ['JPEGImages', 'SegmentationClass', 'ImageSets']\n",
    "            if all(os.path.exists(os.path.join(possible_root, subdir)) for subdir in expected_subdirs):\n",
    "                dataset_root = possible_root\n",
    "                print(f\"\\n‚úÖ Found valid PASCAL VOC structure at: {dataset_root}\")\n",
    "                break\n",
    "            elif any(os.path.exists(os.path.join(possible_root, subdir)) for subdir in expected_subdirs):\n",
    "                dataset_root = possible_root\n",
    "                print(f\"\\n‚ö†Ô∏è  Partial PASCAL VOC structure found at: {dataset_root}\")\n",
    "                break\n",
    "    \n",
    "    if dataset_root is None:\n",
    "        # Try to find any directory containing JPEGImages\n",
    "        print(f\"\\nüîç Searching for JPEGImages directory...\")\n",
    "        for root, dirs, files in os.walk(dataset_path):\n",
    "            if 'JPEGImages' in dirs:\n",
    "                dataset_root = root\n",
    "                print(f\"‚úÖ Found JPEGImages in: {dataset_root}\")\n",
    "                break\n",
    "    \n",
    "    if dataset_root is None:\n",
    "        dataset_root = dataset_path\n",
    "        print(f\"\\n‚ö†Ô∏è  Using base path as dataset root: {dataset_root}\")\n",
    "    \n",
    "    # Update configuration with found paths\n",
    "    CFG.update({\n",
    "        'DATA_ROOT': dataset_root + '/',\n",
    "        'IMAGE_SET_FILE_TRAIN': os.path.join(dataset_root, 'ImageSets', 'Segmentation', 'train.txt'),\n",
    "        'IMAGE_SET_FILE_VAL': os.path.join(dataset_root, 'ImageSets', 'Segmentation', 'val.txt'),\n",
    "    })\n",
    "    \n",
    "    print(f\"\\nüìù Configuration updated with found paths:\")\n",
    "    print(f\"   DATA_ROOT: {CFG['DATA_ROOT']}\")\n",
    "    print(f\"   TRAIN_FILE: {CFG['IMAGE_SET_FILE_TRAIN']}\")\n",
    "    print(f\"   VAL_FILE: {CFG['IMAGE_SET_FILE_VAL']}\")\n",
    "    \n",
    "    # Verify dataset structure\n",
    "    print(f\"\\nüîç Verifying dataset structure...\")\n",
    "    expected_dirs = ['JPEGImages', 'SegmentationClass', 'ImageSets']\n",
    "    \n",
    "    for dir_name in expected_dirs:\n",
    "        dir_path = os.path.join(dataset_root, dir_name)\n",
    "        if os.path.exists(dir_path):\n",
    "            if dir_name == 'JPEGImages':\n",
    "                try:\n",
    "                    files = [f for f in os.listdir(dir_path) if f.endswith(('.jpg', '.jpeg', '.JPG', '.JPEG'))]\n",
    "                    file_count = len(files)\n",
    "                    print(f\"   ‚úÖ {dir_name}: {file_count:,} images found\")\n",
    "                except:\n",
    "                    print(f\"   ‚ö†Ô∏è  {dir_name}: Directory found but couldn't count files\")\n",
    "            elif dir_name == 'SegmentationClass':\n",
    "                try:\n",
    "                    files = [f for f in os.listdir(dir_path) if f.endswith(('.png', '.PNG'))]\n",
    "                    file_count = len(files)\n",
    "                    print(f\"   ‚úÖ {dir_name}: {file_count:,} segmentation masks found\")\n",
    "                except:\n",
    "                    print(f\"   ‚ö†Ô∏è  {dir_name}: Directory found but couldn't count files\")\n",
    "            else:\n",
    "                print(f\"   ‚úÖ {dir_name}: Directory found\")\n",
    "        else:\n",
    "            print(f\"   ‚ùå {dir_name}: Directory not found\")\n",
    "    \n",
    "    # Verify train/val split files\n",
    "    train_file_exists = os.path.exists(CFG['IMAGE_SET_FILE_TRAIN'])\n",
    "    val_file_exists = os.path.exists(CFG['IMAGE_SET_FILE_VAL'])\n",
    "    \n",
    "    if train_file_exists:\n",
    "        try:\n",
    "            with open(CFG['IMAGE_SET_FILE_TRAIN'], 'r') as f:\n",
    "                train_ids = [line.strip() for line in f.readlines()]\n",
    "            print(f\"   ‚úÖ Training split: {len(train_ids):,} samples\")\n",
    "        except:\n",
    "            print(f\"   ‚ö†Ô∏è  Training split file exists but couldn't read\")\n",
    "    else:\n",
    "        print(f\"   ‚ùå Training split file not found\")\n",
    "    \n",
    "    if val_file_exists:\n",
    "        try:\n",
    "            with open(CFG['IMAGE_SET_FILE_VAL'], 'r') as f:\n",
    "                val_ids = [line.strip() for line in f.readlines()]\n",
    "            print(f\"   ‚úÖ Validation split: {len(val_ids):,} samples\")\n",
    "        except:\n",
    "            print(f\"   ‚ö†Ô∏è  Validation split file exists but couldn't read\")\n",
    "    else:\n",
    "        print(f\"   ‚ùå Validation split file not found\")\n",
    "    \n",
    "    if train_file_exists and val_file_exists:\n",
    "        print(f\"\\nüéØ PASCAL VOC 2012 dataset ready for training!\")\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è  Dataset structure incomplete - may need manual verification\")\n",
    "        \n",
    "        # Try to find split files in alternative locations\n",
    "        print(f\"\\nüîç Searching for split files...\")\n",
    "        for root, dirs, files in os.walk(dataset_path):\n",
    "            if 'train.txt' in files or 'val.txt' in files:\n",
    "                print(f\"   Found split files in: {root}\")\n",
    "                for file in files:\n",
    "                    if file in ['train.txt', 'val.txt']:\n",
    "                        print(f\"     üìÑ {file}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error downloading dataset: {e}\")\n",
    "    print(\"üîÑ Please check your internet connection and try again\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377d9213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== DATA AUGMENTATION PIPELINE =====\n",
    "\n",
    "def get_train_transforms():\n",
    "    \"\"\"\n",
    "    Training data augmentation pipeline following DeepLabv3+ paper specifications.\n",
    "    \n",
    "    Implements the exact augmentation strategy from the original paper:\n",
    "    - Random scaling (0.5x to 2.0x)\n",
    "    - Random cropping to target size\n",
    "    - Horizontal flip augmentation\n",
    "    - ImageNet normalization for pretrained backbone\n",
    "    \n",
    "    Returns:\n",
    "        albumentations.Compose: Training transform pipeline\n",
    "    \"\"\"\n",
    "    return A.Compose([\n",
    "        # Random scale augmentation (0.5x to 2.0x) - Paper specification\n",
    "        A.RandomScale(\n",
    "            scale_limit=(CFG['SCALE_MIN'] - 1.0, CFG['SCALE_MAX'] - 1.0), \n",
    "            p=1.0\n",
    "        ),\n",
    "        \n",
    "        # Pad image if needed to ensure minimum crop size\n",
    "        A.PadIfNeeded(\n",
    "            min_height=CFG['CROP_SIZE'],\n",
    "            min_width=CFG['CROP_SIZE'],\n",
    "            border_mode=0,  # cv2.BORDER_CONSTANT with 0 padding\n",
    "            p=1.0\n",
    "        ),\n",
    "        \n",
    "        # Random crop to final input size\n",
    "        A.RandomCrop(\n",
    "            height=CFG['CROP_SIZE'],\n",
    "            width=CFG['CROP_SIZE'],\n",
    "            p=1.0\n",
    "        ),\n",
    "        \n",
    "        # Horizontal flip augmentation\n",
    "        A.HorizontalFlip(p=CFG['FLIP_PROB']),\n",
    "        \n",
    "        # ImageNet normalization (required for pretrained ResNet backbone)\n",
    "        A.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],  # ImageNet statistics\n",
    "            std=[0.229, 0.224, 0.225],\n",
    "            max_pixel_value=255.0\n",
    "        ),\n",
    "        \n",
    "        # Convert to PyTorch tensors\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "\n",
    "\n",
    "def get_val_transforms():\n",
    "    \"\"\"\n",
    "    Validation data transformation pipeline.\n",
    "    \n",
    "    Simple pipeline for consistent evaluation:\n",
    "    - Resize to target input size\n",
    "    - ImageNet normalization\n",
    "    - Convert to tensors\n",
    "    \n",
    "    Returns:\n",
    "        albumentations.Compose: Validation transform pipeline\n",
    "    \"\"\"\n",
    "    return A.Compose([\n",
    "        # Resize to target input size (no random augmentation for validation)\n",
    "        A.Resize(\n",
    "            height=CFG['CROP_SIZE'],\n",
    "            width=CFG['CROP_SIZE']\n",
    "        ),\n",
    "        \n",
    "        # ImageNet normalization\n",
    "        A.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225],\n",
    "            max_pixel_value=255.0\n",
    "        ),\n",
    "        \n",
    "        # Convert to PyTorch tensors\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "\n",
    "\n",
    "# Test and display transform pipelines\n",
    "print(\"üé® DATA AUGMENTATION PIPELINE SETUP\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "train_transforms = get_train_transforms()\n",
    "val_transforms = get_val_transforms()\n",
    "\n",
    "print(f\"‚úÖ Transform pipelines created successfully!\")\n",
    "print(f\"   Training transforms: {len(train_transforms.transforms)} steps\")\n",
    "print(f\"   Validation transforms: {len(val_transforms.transforms)} steps\")\n",
    "\n",
    "print(f\"\\nüìã TRAINING AUGMENTATION PIPELINE:\")\n",
    "for i, transform in enumerate(train_transforms.transforms, 1):\n",
    "    transform_name = transform.__class__.__name__\n",
    "    if hasattr(transform, 'p'):\n",
    "        print(f\"   {i}. {transform_name} (p={transform.p})\")\n",
    "    else:\n",
    "        print(f\"   {i}. {transform_name}\")\n",
    "\n",
    "print(f\"\\nüìã VALIDATION TRANSFORM PIPELINE:\")\n",
    "for i, transform in enumerate(val_transforms.transforms, 1):\n",
    "    transform_name = transform.__class__.__name__\n",
    "    print(f\"   {i}. {transform_name}\")\n",
    "\n",
    "print(f\"\\nüéØ Augmentation parameters:\")\n",
    "print(f\"   - Scale range: {CFG['SCALE_MIN']}x to {CFG['SCALE_MAX']}x\")\n",
    "print(f\"   - Crop size: {CFG['CROP_SIZE']}√ó{CFG['CROP_SIZE']}\")\n",
    "print(f\"   - Horizontal flip probability: {CFG['FLIP_PROB']}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4987f9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== PASCAL VOC DATASET CLASS IMPLEMENTATION =====\n",
    "\n",
    "class PascalVOCDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Custom PyTorch Dataset class for PASCAL VOC 2012 semantic segmentation.\n",
    "    \n",
    "    This dataset class handles:\n",
    "    - Loading RGB images and segmentation masks\n",
    "    - Applying data transformations (augmentation/normalization)\n",
    "    - Ensuring proper data types for PyTorch training\n",
    "    - Handling PASCAL VOC file structure and naming conventions\n",
    "    \n",
    "    Args:\n",
    "        image_set_file (str): Path to train.txt or val.txt file\n",
    "        root_dir (str): Root directory of PASCAL VOC dataset\n",
    "        transforms (albumentations.Compose): Transform pipeline to apply\n",
    "        \n",
    "    Returns:\n",
    "        dict: {'image': tensor, 'mask': tensor} for each sample\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, image_set_file, root_dir, transforms=None):\n",
    "        \"\"\"Initialize the dataset with file paths and transforms.\"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transforms = transforms\n",
    "        \n",
    "        # Read image IDs from the split file\n",
    "        self.image_ids = []\n",
    "        try:\n",
    "            with open(image_set_file, 'r') as f:\n",
    "                self.image_ids = [line.strip() for line in f.readlines()]\n",
    "            \n",
    "            print(f\"üìä Loaded {len(self.image_ids):,} samples from {os.path.basename(image_set_file)}\")\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            print(f\"‚ùå Error: Could not find image set file: {image_set_file}\")\n",
    "            raise\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Return the total number of samples in the dataset.\"\"\"\n",
    "        return len(self.image_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Load and return a single sample (image + mask pair).\n",
    "        \n",
    "        Args:\n",
    "            idx (int): Index of the sample to load\n",
    "            \n",
    "        Returns:\n",
    "            dict: Dictionary containing 'image' and 'mask' tensors\n",
    "        \"\"\"\n",
    "        # Get image ID for this index\n",
    "        image_id = self.image_ids[idx]\n",
    "        \n",
    "        # Construct file paths\n",
    "        img_path = os.path.join(self.root_dir, 'JPEGImages', f'{image_id}.jpg')\n",
    "        mask_path = os.path.join(self.root_dir, 'SegmentationClass', f'{image_id}.png')\n",
    "        \n",
    "        try:\n",
    "            # Load image in RGB mode\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "            image = np.array(image, dtype=np.uint8)\n",
    "            \n",
    "            # Load segmentation mask (palette mode for indexed colors)\n",
    "            mask = Image.open(mask_path).convert('P')\n",
    "            mask = np.array(mask, dtype=np.uint8)\n",
    "            \n",
    "            # Apply transformations if provided\n",
    "            if self.transforms:\n",
    "                # Albumentations expects dict with 'image' and 'mask' keys\n",
    "                transformed = self.transforms(image=image, mask=mask)\n",
    "                image = transformed['image']\n",
    "                mask = transformed['mask']\n",
    "            \n",
    "            # Ensure mask tensor is long type (required for CrossEntropyLoss)\n",
    "            if isinstance(mask, torch.Tensor):\n",
    "                mask = mask.long()\n",
    "            \n",
    "            return {\n",
    "                'image': image,\n",
    "                'mask': mask,\n",
    "                'image_id': image_id  # Keep ID for debugging/visualization\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading sample {image_id}: {e}\")\n",
    "            # Return a dummy sample to avoid crashing the dataloader\n",
    "            dummy_image = torch.zeros(3, CFG['CROP_SIZE'], CFG['CROP_SIZE'])\n",
    "            dummy_mask = torch.zeros(CFG['CROP_SIZE'], CFG['CROP_SIZE'], dtype=torch.long)\n",
    "            return {'image': dummy_image, 'mask': dummy_mask, 'image_id': image_id}\n",
    "\n",
    "\n",
    "# Test the dataset class\n",
    "print(\"üóÇÔ∏è  PASCAL VOC DATASET CLASS TESTING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Test dataset creation without transforms first\n",
    "try:\n",
    "    print(\"Creating test datasets...\")\n",
    "    \n",
    "    # Training dataset (without transforms for initial testing)\n",
    "    train_dataset_test = PascalVOCDataset(\n",
    "        image_set_file=CFG['IMAGE_SET_FILE_TRAIN'],\n",
    "        root_dir=CFG['DATA_ROOT'],\n",
    "        transforms=None\n",
    "    )\n",
    "    \n",
    "    # Validation dataset (without transforms for initial testing)\n",
    "    val_dataset_test = PascalVOCDataset(\n",
    "        image_set_file=CFG['IMAGE_SET_FILE_VAL'],\n",
    "        root_dir=CFG['DATA_ROOT'],\n",
    "        transforms=None\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Dataset creation successful!\")\n",
    "    print(f\"   Training samples: {len(train_dataset_test):,}\")\n",
    "    print(f\"   Validation samples: {len(val_dataset_test):,}\")\n",
    "    \n",
    "    # Test loading a single sample\n",
    "    print(f\"\\nüîç Testing sample loading...\")\n",
    "    sample = train_dataset_test[0]\n",
    "    \n",
    "    print(f\"‚úÖ Sample loaded successfully!\")\n",
    "    print(f\"   Image ID: {sample['image_id']}\")\n",
    "    print(f\"   Image shape: {sample['image'].shape}\")\n",
    "    print(f\"   Image dtype: {sample['image'].dtype}\")\n",
    "    print(f\"   Mask shape: {sample['mask'].shape}\")\n",
    "    print(f\"   Mask dtype: {sample['mask'].dtype}\")\n",
    "    print(f\"   Unique mask values: {len(np.unique(sample['mask']))} classes\")\n",
    "    print(f\"   Mask value range: {np.min(sample['mask'])} to {np.max(sample['mask'])}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error testing dataset: {e}\")\n",
    "    print(\"‚ö†Ô∏è  This error is expected if PASCAL VOC data is not available\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a01fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== DATALOADER CREATION (MEMORY OPTIMIZED) =====\n",
    "\n",
    "print(\"üîÑ DATALOADER CREATION (T4 OPTIMIZED)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create transform instances\n",
    "print(\"Creating data transforms...\")\n",
    "train_transforms = get_train_transforms()\n",
    "val_transforms = get_val_transforms()\n",
    "print(\"‚úÖ Data transforms created\")\n",
    "\n",
    "# Create dataset instances with transforms\n",
    "print(\"\\nInstantiating datasets with transforms...\")\n",
    "train_dataset = PascalVOCDataset(\n",
    "    image_set_file=CFG['IMAGE_SET_FILE_TRAIN'],\n",
    "    root_dir=CFG['DATA_ROOT'],\n",
    "    transforms=train_transforms\n",
    ")\n",
    "\n",
    "val_dataset = PascalVOCDataset(\n",
    "    image_set_file=CFG['IMAGE_SET_FILE_VAL'],\n",
    "    root_dir=CFG['DATA_ROOT'],\n",
    "    transforms=val_transforms\n",
    ")\n",
    "\n",
    "# Create DataLoaders with memory-optimized settings\n",
    "print(\"\\nCreating DataLoaders...\")\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=CFG['BATCH_SIZE'],\n",
    "    shuffle=True,  # Shuffle training data\n",
    "    num_workers=CFG['NUM_WORKERS'],  # Reduced for memory\n",
    "    pin_memory=True,  # Faster GPU transfer\n",
    "    drop_last=True,  # Drop incomplete batches for consistent training\n",
    "    persistent_workers=True if CFG['NUM_WORKERS'] > 0 else False  # Keep workers alive\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=CFG['BATCH_SIZE'],\n",
    "    shuffle=False,  # No shuffling for validation\n",
    "    num_workers=CFG['NUM_WORKERS'],\n",
    "    pin_memory=True,\n",
    "    drop_last=False,  # Keep all validation samples\n",
    "    persistent_workers=True if CFG['NUM_WORKERS'] > 0 else False\n",
    ")\n",
    "\n",
    "print(\"‚úÖ DataLoaders created successfully\")\n",
    "\n",
    "# Calculate training parameters with gradient accumulation\n",
    "batches_per_epoch = len(train_loader)\n",
    "effective_batches_per_epoch = (batches_per_epoch + CFG['GRADIENT_ACCUMULATION_STEPS'] - 1) // CFG['GRADIENT_ACCUMULATION_STEPS']\n",
    "total_epochs_needed = int(np.ceil(CFG['MAX_ITERATIONS'] / effective_batches_per_epoch))\n",
    "\n",
    "# Display comprehensive summary\n",
    "print(f\"\\nüìä DATALOADER CONFIGURATION SUMMARY (T4 OPTIMIZED)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Training Configuration:\")\n",
    "print(f\"  ‚îî‚îÄ Samples: {len(train_dataset):,}\")\n",
    "print(f\"  ‚îî‚îÄ Batch size: {CFG['BATCH_SIZE']} (per GPU)\")\n",
    "print(f\"  ‚îî‚îÄ Gradient accumulation: {CFG['GRADIENT_ACCUMULATION_STEPS']} steps\")\n",
    "print(f\"  ‚îî‚îÄ Effective batch size: {CFG['BATCH_SIZE'] * CFG['GRADIENT_ACCUMULATION_STEPS']}\")\n",
    "print(f\"  ‚îî‚îÄ Batches per epoch: {batches_per_epoch:,}\")\n",
    "print(f\"  ‚îî‚îÄ Effective iterations per epoch: {effective_batches_per_epoch:,}\")\n",
    "print(f\"  ‚îî‚îÄ Shuffle: True\")\n",
    "print(f\"  ‚îî‚îÄ Data workers: {CFG['NUM_WORKERS']}\")\n",
    "\n",
    "print(f\"\\nValidation Configuration:\")\n",
    "print(f\"  ‚îî‚îÄ Samples: {len(val_dataset):,}\")\n",
    "print(f\"  ‚îî‚îÄ Batch size: {CFG['BATCH_SIZE']}\")\n",
    "print(f\"  ‚îî‚îÄ Batches per epoch: {len(val_loader):,}\")\n",
    "print(f\"  ‚îî‚îÄ Shuffle: False\")\n",
    "\n",
    "print(f\"\\nTraining Schedule:\")\n",
    "print(f\"  ‚îî‚îÄ Max iterations: {CFG['MAX_ITERATIONS']:,}\")\n",
    "print(f\"  ‚îî‚îÄ Estimated epochs needed: {total_epochs_needed}\")\n",
    "print(f\"  ‚îî‚îÄ Actual effective iterations: {total_epochs_needed * effective_batches_per_epoch:,}\")\n",
    "\n",
    "print(f\"\\nMemory Optimization:\")\n",
    "print(f\"  ‚îî‚îÄ Input resolution: {CFG['CROP_SIZE']}√ó{CFG['CROP_SIZE']} (reduced from 513)\")\n",
    "print(f\"  ‚îî‚îÄ Batch size: {CFG['BATCH_SIZE']} (reduced from 8)\")\n",
    "print(f\"  ‚îî‚îÄ Workers: {CFG['NUM_WORKERS']} (reduced for memory)\")\n",
    "print(f\"  ‚îî‚îÄ Pin memory: True\")\n",
    "print(f\"  ‚îî‚îÄ Persistent workers: {CFG['NUM_WORKERS'] > 0}\")\n",
    "\n",
    "# Test batch loading with memory monitoring\n",
    "print(f\"\\nüß™ Testing batch loading...\")\n",
    "try:\n",
    "    # Monitor memory before\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        mem_before = torch.cuda.memory_allocated() / 1024**3\n",
    "    \n",
    "    # Test training batch\n",
    "    train_batch = next(iter(train_loader))\n",
    "    print(f\"‚úÖ Training batch loaded successfully\")\n",
    "    print(f\"   ‚îî‚îÄ Image batch: {train_batch['image'].shape} ({train_batch['image'].dtype})\")\n",
    "    print(f\"   ‚îî‚îÄ Mask batch: {train_batch['mask'].shape} ({train_batch['mask'].dtype})\")\n",
    "    \n",
    "    # Monitor memory after\n",
    "    if torch.cuda.is_available():\n",
    "        mem_after = torch.cuda.memory_allocated() / 1024**3\n",
    "        print(f\"   ‚îî‚îÄ Memory usage: {mem_after - mem_before:.3f} GB per batch\")\n",
    "    \n",
    "    # Test validation batch\n",
    "    val_batch = next(iter(val_loader))\n",
    "    print(f\"‚úÖ Validation batch loaded successfully\")\n",
    "    print(f\"   ‚îî‚îÄ Image batch: {val_batch['image'].shape} ({val_batch['image'].dtype})\")\n",
    "    print(f\"   ‚îî‚îÄ Mask batch: {val_batch['mask'].shape} ({val_batch['mask'].dtype})\")\n",
    "    \n",
    "    # Verify tensor ranges\n",
    "    print(f\"\\nüìè Tensor value ranges:\")\n",
    "    print(f\"   ‚îî‚îÄ Images: [{train_batch['image'].min():.3f}, {train_batch['image'].max():.3f}]\")\n",
    "    print(f\"   ‚îî‚îÄ Masks: [{train_batch['mask'].min()}, {train_batch['mask'].max()}]\")\n",
    "    \n",
    "    # Clean up test batches\n",
    "    del train_batch, val_batch\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading batches: {e}\")\n",
    "    print(\"‚ö†Ô∏è  This might indicate memory or data path issues\")\n",
    "\n",
    "# Memory usage estimate\n",
    "estimated_memory_per_batch = CFG['BATCH_SIZE'] * 3 * CFG['CROP_SIZE'] * CFG['CROP_SIZE'] * 4 / 1024**3  # FP32\n",
    "print(f\"\\nüíæ Memory Estimates:\")\n",
    "print(f\"   ‚îî‚îÄ Input batch size: ~{estimated_memory_per_batch:.3f} GB\")\n",
    "print(f\"   ‚îî‚îÄ With model forward: ~{estimated_memory_per_batch * 3:.3f} GB (estimated)\")\n",
    "print(f\"   ‚îî‚îÄ Total training memory: ~{estimated_memory_per_batch * 4:.3f} GB (estimated)\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    print(f\"   ‚îî‚îÄ GPU total memory: {total_memory:.1f} GB\")\n",
    "    memory_utilization = (estimated_memory_per_batch * 4 / total_memory) * 100\n",
    "    print(f\"   ‚îî‚îÄ Estimated utilization: {memory_utilization:.1f}%\")\n",
    "    \n",
    "    if memory_utilization > 80:\n",
    "        print(f\"   ‚ö†Ô∏è  High memory utilization - consider reducing batch size\")\n",
    "\n",
    "print(f\"\\nüöÄ DataLoader setup complete and ready for training!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9602048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== MODEL DEFINITION AND CONFIGURATION =====\n",
    "\n",
    "print(\"üèóÔ∏è  MODEL DEFINITION AND CONFIGURATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load DeepLabv3+ with ResNet-101 backbone from torchvision\n",
    "print(\"Loading DeepLabv3+ model with ResNet-101 backbone...\")\n",
    "model = torchvision.models.segmentation.deeplabv3_resnet101(\n",
    "    weights='DeepLabV3_ResNet101_Weights.DEFAULT'  # Use pretrained weights\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Base model loaded successfully\")\n",
    "print(f\"   Original classifier output channels: {model.classifier[4].out_channels}\")\n",
    "\n",
    "# Modify classifier head for PASCAL VOC (21 classes)\n",
    "model.classifier[4] = nn.Conv2d(\n",
    "    in_channels=256,\n",
    "    out_channels=CFG['NUM_CLASSES'],\n",
    "    kernel_size=(1, 1),\n",
    "    stride=(1, 1)\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Main classifier head updated for {CFG['NUM_CLASSES']} classes\")\n",
    "\n",
    "# Modify auxiliary classifier if present\n",
    "if hasattr(model, 'aux_classifier') and model.aux_classifier is not None:\n",
    "    model.aux_classifier[4] = nn.Conv2d(\n",
    "        in_channels=256,\n",
    "        out_channels=CFG['NUM_CLASSES'],\n",
    "        kernel_size=(1, 1),\n",
    "        stride=(1, 1)\n",
    "    )\n",
    "    print(f\"‚úÖ Auxiliary classifier head updated for {CFG['NUM_CLASSES']} classes\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è  No auxiliary classifier found (this is normal)\")\n",
    "\n",
    "# Move model to the appropriate device\n",
    "model = model.to(CFG['DEVICE'])\n",
    "print(f\"‚úÖ Model moved to device: {CFG['DEVICE']}\")\n",
    "\n",
    "# Calculate model parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nüìä MODEL ARCHITECTURE SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Model: DeepLabv3+ with {CFG['BACKBONE']} backbone\")\n",
    "print(f\"Architecture Details:\")\n",
    "print(f\"  ‚îî‚îÄ Backbone: ResNet-101 (pretrained on ImageNet)\")\n",
    "print(f\"  ‚îî‚îÄ Output stride: {CFG['OUTPUT_STRIDE']}\")\n",
    "print(f\"  ‚îî‚îÄ Number of classes: {CFG['NUM_CLASSES']}\")\n",
    "print(f\"  ‚îî‚îÄ Input resolution: {CFG['CROP_SIZE']}√ó{CFG['CROP_SIZE']}\")\n",
    "\n",
    "print(f\"\\nModel Parameters:\")\n",
    "print(f\"  ‚îî‚îÄ Total parameters: {total_params:,}\")\n",
    "print(f\"  ‚îî‚îÄ Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"  ‚îî‚îÄ Model size: ~{total_params * 4 / 1024**2:.1f} MB (FP32)\")\n",
    "\n",
    "# Test model forward pass\n",
    "print(f\"\\nüß™ Testing model forward pass...\")\n",
    "try:\n",
    "    # Create dummy input tensor\n",
    "    dummy_input = torch.randn(1, 3, CFG['CROP_SIZE'], CFG['CROP_SIZE']).to(CFG['DEVICE'])\n",
    "    \n",
    "    # Set model to evaluation mode for testing\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(dummy_input)\n",
    "    \n",
    "    # Check output format\n",
    "    main_output = output['out']\n",
    "    \n",
    "    print(f\"‚úÖ Forward pass successful!\")\n",
    "    print(f\"   Input shape: {dummy_input.shape}\")\n",
    "    print(f\"   Output shape: {main_output.shape}\")\n",
    "    \n",
    "    # Verify output dimensions\n",
    "    expected_shape = (1, CFG['NUM_CLASSES'], CFG['CROP_SIZE'], CFG['CROP_SIZE'])\n",
    "    if main_output.shape == expected_shape:\n",
    "        print(f\"‚úÖ Output shape matches expected: {expected_shape}\")\n",
    "    else:\n",
    "        print(f\"‚ùå Shape mismatch! Expected: {expected_shape}, Got: {main_output.shape}\")\n",
    "    \n",
    "    # Check auxiliary output if present\n",
    "    if 'aux' in output:\n",
    "        aux_output = output['aux']\n",
    "        print(f\"   Auxiliary output shape: {aux_output.shape}\")\n",
    "    \n",
    "    # Check output value ranges\n",
    "    print(f\"   Output value range: [{main_output.min():.3f}, {main_output.max():.3f}]\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Forward pass failed: {e}\")\n",
    "\n",
    "# Set model back to training mode\n",
    "model.train()\n",
    "print(f\"\\n‚úÖ Model definition complete and ready for training!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd8331d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== TRAINING COMPONENTS SETUP =====\n",
    "\n",
    "print(\"‚öôÔ∏è  TRAINING COMPONENTS SETUP (MEMORY OPTIMIZED)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# === LOSS FUNCTION ===\n",
    "# CrossEntropyLoss with ignore_index for unlabeled pixels (value 255)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=CFG['IGNORE_INDEX'])\n",
    "print(f\"‚úÖ Loss function: CrossEntropyLoss\")\n",
    "print(f\"   ‚îî‚îÄ Ignore index: {CFG['IGNORE_INDEX']} (unlabeled pixels)\")\n",
    "\n",
    "# === OPTIMIZER ===\n",
    "# SGD optimizer as specified in the DeepLabv3+ paper\n",
    "optimizer = optim.SGD(\n",
    "    model.parameters(),\n",
    "    lr=CFG['BASE_LR'],\n",
    "    momentum=CFG['MOMENTUM'],\n",
    "    weight_decay=CFG['WEIGHT_DECAY']\n",
    ")\n",
    "print(f\"‚úÖ Optimizer: SGD (as per paper specification)\")\n",
    "print(f\"   ‚îî‚îÄ Learning rate: {CFG['BASE_LR']} (adjusted for smaller batch)\")\n",
    "print(f\"   ‚îî‚îÄ Momentum: {CFG['MOMENTUM']}\")\n",
    "print(f\"   ‚îî‚îÄ Weight decay: {CFG['WEIGHT_DECAY']}\")\n",
    "\n",
    "# === LEARNING RATE SCHEDULER ===\n",
    "# Polynomial learning rate scheduler (step-based, not epoch-based)\n",
    "def poly_lr_lambda(iteration):\n",
    "    \"\"\"Polynomial learning rate decay as per DeepLabv3+ paper.\"\"\"\n",
    "    return (1 - iteration / CFG['MAX_ITERATIONS']) ** CFG['POLY_POWER']\n",
    "\n",
    "scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=poly_lr_lambda)\n",
    "print(f\"‚úÖ LR Scheduler: Polynomial decay (step-based)\")\n",
    "print(f\"   ‚îî‚îÄ Polynomial power: {CFG['POLY_POWER']}\")\n",
    "print(f\"   ‚îî‚îÄ Max iterations: {CFG['MAX_ITERATIONS']:,}\")\n",
    "print(f\"   ‚îî‚îÄ Update frequency: Every {CFG['GRADIENT_ACCUMULATION_STEPS']} batches\")\n",
    "\n",
    "# === MIXED PRECISION SCALER ===\n",
    "scaler = None\n",
    "if CFG['MIXED_PRECISION'] and torch.cuda.is_available():\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    print(f\"‚úÖ Mixed Precision: Enabled (FP16)\")\n",
    "    print(f\"   ‚îî‚îÄ GradScaler initialized for stable training\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  Mixed Precision: Disabled\")\n",
    "\n",
    "# === TRAINING STATE TRACKING ===\n",
    "training_state = {\n",
    "    'best_mIoU': 0.0,\n",
    "    'current_iteration': 0,\n",
    "    'epoch': 0,\n",
    "    'training_history': {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'val_mIoU': [],\n",
    "        'learning_rates': [],\n",
    "        'iterations': [],\n",
    "        'gpu_memory': []\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"‚úÖ Training state tracking initialized\")\n",
    "\n",
    "# === MEMORY MANAGEMENT ===\n",
    "def monitor_gpu_memory():\n",
    "    \"\"\"Monitor and return GPU memory usage\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "        reserved = torch.cuda.memory_reserved() / 1024**3\n",
    "        return {'allocated': allocated, 'reserved': reserved}\n",
    "    return {'allocated': 0, 'reserved': 0}\n",
    "\n",
    "# Clear GPU memory before training\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"‚úÖ GPU memory cleared before training\")\n",
    "\n",
    "# === TRAINING COMPONENTS SUMMARY ===\n",
    "print(f\"\\nüìã TRAINING COMPONENTS SUMMARY (T4 OPTIMIZED)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Loss Function:\")\n",
    "print(f\"  ‚îî‚îÄ CrossEntropyLoss (ignore_index={CFG['IGNORE_INDEX']})\")\n",
    "\n",
    "print(f\"\\nOptimizer:\")\n",
    "print(f\"  ‚îî‚îÄ SGD\")\n",
    "print(f\"  ‚îî‚îÄ Learning Rate: {CFG['BASE_LR']} (adjusted for batch size)\")\n",
    "print(f\"  ‚îî‚îÄ Momentum: {CFG['MOMENTUM']}\")\n",
    "print(f\"  ‚îî‚îÄ Weight Decay: {CFG['WEIGHT_DECAY']}\")\n",
    "\n",
    "print(f\"\\nLR Scheduler:\")\n",
    "print(f\"  ‚îî‚îÄ Polynomial decay (power={CFG['POLY_POWER']})\")\n",
    "print(f\"  ‚îî‚îÄ Step-based updates (per effective iteration)\")\n",
    "print(f\"  ‚îî‚îÄ Max iterations: {CFG['MAX_ITERATIONS']:,}\")\n",
    "\n",
    "print(f\"\\nMemory Optimization:\")\n",
    "print(f\"  ‚îî‚îÄ Batch size: {CFG['BATCH_SIZE']}\")\n",
    "print(f\"  ‚îî‚îÄ Gradient accumulation: {CFG['GRADIENT_ACCUMULATION_STEPS']} steps\")\n",
    "print(f\"  ‚îî‚îÄ Effective batch size: {CFG['BATCH_SIZE'] * CFG['GRADIENT_ACCUMULATION_STEPS']}\")\n",
    "print(f\"  ‚îî‚îÄ Input resolution: {CFG['CROP_SIZE']}√ó{CFG['CROP_SIZE']}\")\n",
    "print(f\"  ‚îî‚îÄ Mixed precision: {CFG['MIXED_PRECISION']}\")\n",
    "print(f\"  ‚îî‚îÄ Data workers: {CFG['NUM_WORKERS']}\")\n",
    "\n",
    "# === TEST LEARNING RATE SCHEDULE ===\n",
    "print(f\"\\nüìà Learning Rate Schedule Preview:\")\n",
    "print(\"Iteration ‚Üí Learning Rate\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "test_iterations = [0, 1000, 5000, 10000, 15000, 20000]\n",
    "for iteration in test_iterations:\n",
    "    if iteration <= CFG['MAX_ITERATIONS']:\n",
    "        lr_multiplier = poly_lr_lambda(iteration)\n",
    "        effective_lr = CFG['BASE_LR'] * lr_multiplier\n",
    "        print(f\"{iteration:>8,} ‚Üí {effective_lr:.6f}\")\n",
    "\n",
    "# Show current memory usage\n",
    "memory_info = monitor_gpu_memory()\n",
    "print(f\"\\nüíæ Current GPU Memory Usage:\")\n",
    "print(f\"   ‚îî‚îÄ Allocated: {memory_info['allocated']:.2f} GB\")\n",
    "print(f\"   ‚îî‚îÄ Reserved: {memory_info['reserved']:.2f} GB\")\n",
    "\n",
    "print(f\"\\n‚úÖ Training components setup complete!\")\n",
    "print(f\"‚ö†Ô∏è  Note: Using gradient accumulation to simulate larger batch sizes\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ee34c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== METRICS IMPLEMENTATION (mIoU) =====\n",
    "\n",
    "def compute_miou(predictions, targets, num_classes, ignore_index):\n",
    "    \"\"\"\n",
    "    Compute mean Intersection over Union (mIoU) for semantic segmentation.\n",
    "    \n",
    "    This is the standard evaluation metric for semantic segmentation tasks.\n",
    "    mIoU calculates the IoU for each class and then averages across all classes.\n",
    "    \n",
    "    Args:\n",
    "        predictions (torch.Tensor): Model logits with shape (B, C, H, W)\n",
    "        targets (torch.Tensor): Ground truth masks with shape (B, H, W)\n",
    "        num_classes (int): Number of classes in the dataset\n",
    "        ignore_index (int): Index to ignore (typically 255 for unlabeled pixels)\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (mean_iou, per_class_iou)\n",
    "            - mean_iou (float): Average IoU across all valid classes\n",
    "            - per_class_iou (np.ndarray): IoU for each individual class\n",
    "    \"\"\"\n",
    "    # Move to CPU and convert to numpy for efficient computation\n",
    "    predictions = predictions.detach().cpu()\n",
    "    targets = targets.detach().cpu()\n",
    "    \n",
    "    # Get class predictions by taking argmax over channel dimension\n",
    "    pred_labels = torch.argmax(predictions, dim=1)  # Shape: (B, H, W)\n",
    "    \n",
    "    # Flatten arrays for easier processing\n",
    "    pred_flat = pred_labels.flatten().numpy()\n",
    "    target_flat = targets.flatten().numpy()\n",
    "    \n",
    "    # Create mask to exclude ignore_index pixels\n",
    "    valid_mask = (target_flat != ignore_index)\n",
    "    pred_flat = pred_flat[valid_mask]\n",
    "    target_flat = target_flat[valid_mask]\n",
    "    \n",
    "    # Initialize arrays for intersection and union counts\n",
    "    intersections = np.zeros(num_classes, dtype=np.float32)\n",
    "    unions = np.zeros(num_classes, dtype=np.float32)\n",
    "    \n",
    "    # Compute intersection and union for each class\n",
    "    for class_id in range(num_classes):\n",
    "        # Intersection: pixels correctly predicted as this class\n",
    "        intersection = np.sum((pred_flat == class_id) & (target_flat == class_id))\n",
    "        \n",
    "        # Union: pixels predicted OR actually belonging to this class\n",
    "        union = np.sum((pred_flat == class_id) | (target_flat == class_id))\n",
    "        \n",
    "        intersections[class_id] = intersection\n",
    "        unions[class_id] = union\n",
    "    \n",
    "    # Compute IoU per class (handle division by zero)\n",
    "    per_class_iou = np.zeros(num_classes, dtype=np.float32)\n",
    "    valid_classes = []\n",
    "    \n",
    "    for class_id in range(num_classes):\n",
    "        if unions[class_id] > 0:\n",
    "            per_class_iou[class_id] = intersections[class_id] / unions[class_id]\n",
    "            valid_classes.append(class_id)\n",
    "        else:\n",
    "            per_class_iou[class_id] = float('nan')  # No samples for this class\n",
    "    \n",
    "    # Compute mean IoU across valid classes\n",
    "    if len(valid_classes) > 0:\n",
    "        mean_iou = np.nanmean(per_class_iou)\n",
    "    else:\n",
    "        mean_iou = 0.0\n",
    "    \n",
    "    return mean_iou, per_class_iou\n",
    "\n",
    "\n",
    "def compute_detailed_miou(predictions, targets, num_classes, ignore_index, class_names=None):\n",
    "    \"\"\"\n",
    "    Compute detailed mIoU with per-class breakdown and class names.\n",
    "    \n",
    "    Args:\n",
    "        predictions (torch.Tensor): Model predictions\n",
    "        targets (torch.Tensor): Ground truth\n",
    "        num_classes (int): Number of classes\n",
    "        ignore_index (int): Index to ignore\n",
    "        class_names (list): Optional list of class names for display\n",
    "    \n",
    "    Returns:\n",
    "        dict: Detailed results with mIoU and per-class IoU\n",
    "    \"\"\"\n",
    "    mean_iou, per_class_iou = compute_miou(predictions, targets, num_classes, ignore_index)\n",
    "    \n",
    "    # Use provided class names or generate generic ones\n",
    "    if class_names is None:\n",
    "        class_names = [f'Class_{i}' for i in range(num_classes)]\n",
    "    \n",
    "    # Create detailed results dictionary\n",
    "    results = {\n",
    "        'mIoU': mean_iou,\n",
    "        'per_class_IoU': {},\n",
    "        'valid_classes': 0,\n",
    "        'total_classes': num_classes\n",
    "    }\n",
    "    \n",
    "    valid_count = 0\n",
    "    for class_id in range(num_classes):\n",
    "        class_name = class_names[class_id] if class_id < len(class_names) else f'Class_{class_id}'\n",
    "        iou_value = per_class_iou[class_id]\n",
    "        \n",
    "        results['per_class_IoU'][class_name] = iou_value\n",
    "        \n",
    "        if not np.isnan(iou_value):\n",
    "            valid_count += 1\n",
    "    \n",
    "    results['valid_classes'] = valid_count\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# Test mIoU computation\n",
    "print(\"üìä mIoU METRICS IMPLEMENTATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"Testing mIoU computation with dummy data...\")\n",
    "\n",
    "# Create test data\n",
    "batch_size, height, width = 2, 64, 64\n",
    "num_test_classes = CFG['NUM_CLASSES']\n",
    "\n",
    "# Generate dummy predictions (logits)\n",
    "dummy_predictions = torch.randn(batch_size, num_test_classes, height, width)\n",
    "\n",
    "# Generate dummy targets with some ignore_index pixels\n",
    "dummy_targets = torch.randint(0, num_test_classes, (batch_size, height, width))\n",
    "# Add some ignore_index pixels (simulate unlabeled regions)\n",
    "dummy_targets[dummy_targets > num_test_classes - 3] = CFG['IGNORE_INDEX']\n",
    "\n",
    "print(f\"‚úÖ Test data created:\")\n",
    "print(f\"   ‚îî‚îÄ Predictions shape: {dummy_predictions.shape}\")\n",
    "print(f\"   ‚îî‚îÄ Targets shape: {dummy_targets.shape}\")\n",
    "print(f\"   ‚îî‚îÄ Unique target values: {torch.unique(dummy_targets).tolist()}\")\n",
    "\n",
    "# Test mIoU computation\n",
    "try:\n",
    "    mean_iou, per_class_iou = compute_miou(\n",
    "        dummy_predictions, dummy_targets, num_test_classes, CFG['IGNORE_INDEX']\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ mIoU computation successful!\")\n",
    "    print(f\"   ‚îî‚îÄ Mean IoU: {mean_iou:.4f}\")\n",
    "    print(f\"   ‚îî‚îÄ Per-class IoU shape: {per_class_iou.shape}\")\n",
    "    print(f\"   ‚îî‚îÄ Valid classes: {np.sum(~np.isnan(per_class_iou))}/{num_test_classes}\")\n",
    "    \n",
    "    # Test detailed mIoU\n",
    "    detailed_results = compute_detailed_miou(\n",
    "        dummy_predictions, dummy_targets, num_test_classes, CFG['IGNORE_INDEX'], CFG['VOC_CLASSES']\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Detailed mIoU computation successful!\")\n",
    "    print(f\"   ‚îî‚îÄ Mean IoU: {detailed_results['mIoU']:.4f}\")\n",
    "    print(f\"   ‚îî‚îÄ Valid classes: {detailed_results['valid_classes']}/{detailed_results['total_classes']}\")\n",
    "    \n",
    "    # Show a few per-class IoUs\n",
    "    print(f\"   ‚îî‚îÄ Sample class IoUs:\")\n",
    "    for i, (class_name, iou_val) in enumerate(list(detailed_results['per_class_IoU'].items())[:5]):\n",
    "        if not np.isnan(iou_val):\n",
    "            print(f\"      ‚Ä¢ {class_name}: {iou_val:.4f}\")\n",
    "        else:\n",
    "            print(f\"      ‚Ä¢ {class_name}: N/A (no samples)\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error in mIoU computation: {e}\")\n",
    "\n",
    "print(f\"\\n‚úÖ mIoU metric functions ready for training!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7d51cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== TRAINING AND EVALUATION FUNCTIONS =====\n",
    "\n",
    "def train_one_epoch(model, dataloader, optimizer, scheduler, criterion, device, current_iter, scaler=None):\n",
    "    \"\"\"\n",
    "    Train the model for one epoch with gradient accumulation and mixed precision.\n",
    "    \n",
    "    Args:\n",
    "        model: DeepLabv3+ model\n",
    "        dataloader: Training data loader\n",
    "        optimizer: SGD optimizer\n",
    "        scheduler: Polynomial LR scheduler  \n",
    "        criterion: CrossEntropyLoss\n",
    "        device: Training device (cuda/cpu)\n",
    "        current_iter: Current iteration count\n",
    "        scaler: GradScaler for mixed precision\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (average_loss, updated_iteration_count)\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    num_batches = len(dataloader)\n",
    "    accumulation_steps = CFG['GRADIENT_ACCUMULATION_STEPS']\n",
    "    \n",
    "    # Training loop with progress bar\n",
    "    pbar = tqdm(dataloader, desc=\"üöÇ Training\", leave=False, \n",
    "                bar_format='{l_bar}{bar:30}{r_bar}{bar:-30b}')\n",
    "    \n",
    "    # Initialize gradient accumulation\n",
    "    optimizer.zero_grad()\n",
    "    accumulated_loss = 0.0\n",
    "    \n",
    "    for batch_idx, batch in enumerate(pbar):\n",
    "        # Move data to device\n",
    "        images = batch['image'].to(device, non_blocking=True)\n",
    "        masks = batch['mask'].to(device, non_blocking=True)\n",
    "        \n",
    "        # Mixed precision forward pass\n",
    "        if scaler is not None:\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(images)['out']\n",
    "                loss = criterion(outputs, masks)\n",
    "                # Scale loss for gradient accumulation\n",
    "                loss = loss / accumulation_steps\n",
    "        else:\n",
    "            outputs = model(images)['out']\n",
    "            loss = criterion(outputs, masks)\n",
    "            loss = loss / accumulation_steps\n",
    "        \n",
    "        # Backward pass\n",
    "        if scaler is not None:\n",
    "            scaler.scale(loss).backward()\n",
    "        else:\n",
    "            loss.backward()\n",
    "        \n",
    "        accumulated_loss += loss.item()\n",
    "        \n",
    "        # Update weights every accumulation_steps\n",
    "        if (batch_idx + 1) % accumulation_steps == 0 or (batch_idx + 1) == len(dataloader):\n",
    "            if scaler is not None:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                optimizer.step()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Update learning rate (per effective iteration)\n",
    "            scheduler.step()\n",
    "            current_iter += 1\n",
    "            \n",
    "            # Track running loss (multiply back by accumulation_steps for true loss)\n",
    "            running_loss += accumulated_loss * accumulation_steps\n",
    "            \n",
    "            # Update progress bar\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            pbar.set_postfix({\n",
    "                'Loss': f'{accumulated_loss * accumulation_steps:.4f}',\n",
    "                'LR': f'{current_lr:.2e}',\n",
    "                'Iter': f'{current_iter:,}'\n",
    "            })\n",
    "            \n",
    "            # Clear GPU memory periodically\n",
    "            if current_iter % 100 == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "            \n",
    "            # Reset accumulated loss\n",
    "            accumulated_loss = 0.0\n",
    "    \n",
    "    # Calculate average loss\n",
    "    effective_batches = (num_batches + accumulation_steps - 1) // accumulation_steps\n",
    "    avg_loss = running_loss / max(effective_batches, 1)\n",
    "    \n",
    "    return avg_loss, current_iter\n",
    "\n",
    "\n",
    "def evaluate_model(model, dataloader, criterion, device, num_classes, ignore_index):\n",
    "    \"\"\"\n",
    "    Evaluate the model on validation data with memory optimization.\n",
    "    \n",
    "    Args:\n",
    "        model: DeepLabv3+ model\n",
    "        dataloader: Validation data loader\n",
    "        criterion: Loss function\n",
    "        device: Computation device\n",
    "        num_classes: Number of segmentation classes\n",
    "        ignore_index: Index to ignore in metrics\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (avg_val_loss, avg_miou)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    running_val_loss = 0.0\n",
    "    total_miou = 0.0\n",
    "    num_batches = len(dataloader)\n",
    "    \n",
    "    # Clear GPU memory before evaluation\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Evaluation loop without gradient computation\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(dataloader, desc=\"üìä Evaluating\", leave=False,\n",
    "                    bar_format='{l_bar}{bar:30}{r_bar}{bar:-30b}')\n",
    "        \n",
    "        for batch_idx, batch in enumerate(pbar):\n",
    "            # Move data to device\n",
    "            images = batch['image'].to(device, non_blocking=True)\n",
    "            masks = batch['mask'].to(device, non_blocking=True)\n",
    "            \n",
    "            # Forward pass with mixed precision\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(images)['out']\n",
    "                val_loss = criterion(outputs, masks)\n",
    "            \n",
    "            running_val_loss += val_loss.item()\n",
    "            \n",
    "            # Compute mIoU for this batch\n",
    "            batch_miou, _ = compute_miou(outputs, masks, num_classes, ignore_index)\n",
    "            total_miou += batch_miou\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({\n",
    "                'Val Loss': f'{val_loss.item():.4f}',\n",
    "                'Batch mIoU': f'{batch_miou:.4f}'\n",
    "            })\n",
    "            \n",
    "            # Clear intermediate results to save memory\n",
    "            del outputs, val_loss\n",
    "            if batch_idx % 50 == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "    \n",
    "    # Calculate averages\n",
    "    avg_val_loss = running_val_loss / num_batches\n",
    "    avg_miou = total_miou / num_batches\n",
    "    \n",
    "    return avg_val_loss, avg_miou\n",
    "\n",
    "\n",
    "def save_checkpoint(model, optimizer, scheduler, training_state, filepath, is_best=False):\n",
    "    \"\"\"\n",
    "    Save model checkpoint with training state.\n",
    "    \n",
    "    Args:\n",
    "        model: Model to save\n",
    "        optimizer: Optimizer state\n",
    "        scheduler: Scheduler state\n",
    "        training_state: Training state dictionary\n",
    "        filepath: Path to save checkpoint\n",
    "        is_best: Whether this is the best model so far\n",
    "    \"\"\"\n",
    "    checkpoint = {\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'training_state': training_state,\n",
    "        'config': CFG,  # Save configuration\n",
    "        'is_best': is_best\n",
    "    }\n",
    "    \n",
    "    torch.save(checkpoint, filepath)\n",
    "    \n",
    "    if is_best:\n",
    "        best_filepath = filepath.replace('.pth', '_best.pth')\n",
    "        torch.save(checkpoint, best_filepath)\n",
    "\n",
    "\n",
    "def print_epoch_summary(epoch, train_loss, val_loss, val_miou, current_lr, \n",
    "                       current_iter, best_miou, max_iterations):\n",
    "    \"\"\"\n",
    "    Print comprehensive epoch training summary with memory info.\n",
    "    \n",
    "    Args:\n",
    "        epoch: Current epoch number\n",
    "        train_loss: Average training loss\n",
    "        val_loss: Average validation loss  \n",
    "        val_miou: Average validation mIoU\n",
    "        current_lr: Current learning rate\n",
    "        current_iter: Current iteration count\n",
    "        best_miou: Best mIoU achieved so far\n",
    "        max_iterations: Maximum training iterations\n",
    "    \"\"\"\n",
    "    progress_pct = (current_iter / max_iterations) * 100\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"üìà EPOCH {epoch} RESULTS SUMMARY\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Training Loss:     {train_loss:.6f}\")\n",
    "    print(f\"Validation Loss:   {val_loss:.6f}\")\n",
    "    print(f\"Validation mIoU:   {val_miou:.6f}\")\n",
    "    print(f\"Best mIoU:         {best_miou:.6f}\")\n",
    "    print(f\"Current LR:        {current_lr:.8f}\")\n",
    "    print(f\"Iteration:         {current_iter:,} / {max_iterations:,}\")\n",
    "    print(f\"Progress:          {progress_pct:.1f}%\")\n",
    "    \n",
    "    # Memory information\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "        reserved = torch.cuda.memory_reserved() / 1024**3\n",
    "        print(f\"GPU Memory:        {allocated:.2f}GB allocated, {reserved:.2f}GB reserved\")\n",
    "    \n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "\n",
    "# Test training and evaluation functions\n",
    "print(\"üèãÔ∏è  TRAINING AND EVALUATION FUNCTIONS (MEMORY OPTIMIZED)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"‚úÖ Function definitions complete:\")\n",
    "print(\"   ‚îî‚îÄ train_one_epoch(): Training with gradient accumulation & mixed precision\")\n",
    "print(\"   ‚îî‚îÄ evaluate_model(): Memory-optimized validation with mIoU\")\n",
    "print(\"   ‚îî‚îÄ save_checkpoint(): Comprehensive model and state saving\")\n",
    "print(\"   ‚îî‚îÄ print_epoch_summary(): Results display with memory info\")\n",
    "\n",
    "print(f\"\\nüìã Memory Optimization Features:\")\n",
    "print(\"   ‚îî‚îÄ Gradient accumulation for effective larger batch size\")\n",
    "print(\"   ‚îî‚îÄ Mixed precision training (FP16)\")\n",
    "print(\"   ‚îî‚îÄ Periodic GPU memory clearing\")\n",
    "print(\"   ‚îî‚îÄ Memory-efficient evaluation\")\n",
    "print(\"   ‚îî‚îÄ Non-blocking data transfer\")\n",
    "\n",
    "print(f\"\\nüéØ Training Configuration:\")\n",
    "print(f\"   ‚îî‚îÄ Batch size: {CFG['BATCH_SIZE']}\")\n",
    "print(f\"   ‚îî‚îÄ Gradient accumulation: {CFG['GRADIENT_ACCUMULATION_STEPS']} steps\")\n",
    "print(f\"   ‚îî‚îÄ Effective batch size: {CFG['BATCH_SIZE'] * CFG['GRADIENT_ACCUMULATION_STEPS']}\")\n",
    "print(f\"   ‚îî‚îÄ Input resolution: {CFG['CROP_SIZE']}√ó{CFG['CROP_SIZE']}\")\n",
    "print(f\"   ‚îî‚îÄ Mixed precision: {CFG['MIXED_PRECISION']}\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ce9a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== MAIN TRAINING LOOP (MEMORY OPTIMIZED) =====\n",
    "\n",
    "print(\"üöÄ STARTING DEEPLABV3+ TRAINING (T4 OPTIMIZED)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Initialize training\n",
    "best_mIoU = 0.0\n",
    "current_iter = 0\n",
    "start_epoch = 0\n",
    "\n",
    "# Calculate training schedule\n",
    "batches_per_epoch = len(train_loader)\n",
    "effective_batches_per_epoch = (batches_per_epoch + CFG['GRADIENT_ACCUMULATION_STEPS'] - 1) // CFG['GRADIENT_ACCUMULATION_STEPS']\n",
    "total_epochs_needed = int(np.ceil(CFG['MAX_ITERATIONS'] / effective_batches_per_epoch))\n",
    "\n",
    "# Clear GPU memory before training\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"üìã Training Configuration (T4 Optimized):\")\n",
    "print(f\"   ‚îî‚îÄ Model: DeepLabv3+ with ResNet-101\")\n",
    "print(f\"   ‚îî‚îÄ Dataset: PASCAL VOC 2012 ({len(train_dataset):,} train, {len(val_dataset):,} val)\")\n",
    "print(f\"   ‚îî‚îÄ Batch Size: {CFG['BATCH_SIZE']} (per GPU)\")\n",
    "print(f\"   ‚îî‚îÄ Gradient Accumulation: {CFG['GRADIENT_ACCUMULATION_STEPS']} steps\")\n",
    "print(f\"   ‚îî‚îÄ Effective Batch Size: {CFG['BATCH_SIZE'] * CFG['GRADIENT_ACCUMULATION_STEPS']}\")\n",
    "print(f\"   ‚îî‚îÄ Input Resolution: {CFG['CROP_SIZE']}√ó{CFG['CROP_SIZE']}\")\n",
    "print(f\"   ‚îî‚îÄ Max Iterations: {CFG['MAX_ITERATIONS']:,}\")\n",
    "print(f\"   ‚îî‚îÄ Batches per Epoch: {batches_per_epoch:,}\")\n",
    "print(f\"   ‚îî‚îÄ Effective Iterations per Epoch: {effective_batches_per_epoch:,}\")\n",
    "print(f\"   ‚îî‚îÄ Estimated Epochs: {total_epochs_needed}\")\n",
    "print(f\"   ‚îî‚îÄ Initial Learning Rate: {CFG['BASE_LR']}\")\n",
    "print(f\"   ‚îî‚îÄ Mixed Precision: {CFG['MIXED_PRECISION']}\")\n",
    "print(f\"   ‚îî‚îÄ Device: {CFG['DEVICE']}\")\n",
    "\n",
    "# Show memory info before training\n",
    "memory_info = monitor_gpu_memory()\n",
    "print(f\"   ‚îî‚îÄ GPU Memory: {memory_info['allocated']:.2f}GB allocated, {memory_info['reserved']:.2f}GB reserved\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Main training loop with error handling\n",
    "try:\n",
    "    for epoch in range(start_epoch, total_epochs_needed):\n",
    "        print(f\"\\nüîÑ EPOCH {epoch + 1}/{total_epochs_needed}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # === TRAINING PHASE ===\n",
    "        print(\"üöÇ Training phase...\")\n",
    "        try:\n",
    "            train_loss, current_iter = train_one_epoch(\n",
    "                model=model,\n",
    "                dataloader=train_loader,\n",
    "                optimizer=optimizer,\n",
    "                scheduler=scheduler,\n",
    "                criterion=criterion,\n",
    "                device=CFG['DEVICE'],\n",
    "                current_iter=current_iter,\n",
    "                scaler=scaler\n",
    "            )\n",
    "            \n",
    "            print(f\"‚úÖ Training phase completed - Loss: {train_loss:.6f}\")\n",
    "            \n",
    "        except torch.cuda.OutOfMemoryError as e:\n",
    "            print(f\"‚ùå CUDA OOM during training: {e}\")\n",
    "            print(\"üîß Attempting recovery...\")\n",
    "            \n",
    "            # Clear GPU memory and reduce batch size if possible\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            \n",
    "            print(\"‚ö†Ô∏è  Try reducing BATCH_SIZE or CROP_SIZE in configuration\")\n",
    "            break\n",
    "        \n",
    "        # === VALIDATION PHASE ===\n",
    "        print(\"üìä Validation phase...\")\n",
    "        try:\n",
    "            val_loss, val_mIoU = evaluate_model(\n",
    "                model=model,\n",
    "                dataloader=val_loader,\n",
    "                criterion=criterion,\n",
    "                device=CFG['DEVICE'],\n",
    "                num_classes=CFG['NUM_CLASSES'],\n",
    "                ignore_index=CFG['IGNORE_INDEX']\n",
    "            )\n",
    "            \n",
    "            print(f\"‚úÖ Validation phase completed - Loss: {val_loss:.6f}, mIoU: {val_mIoU:.6f}\")\n",
    "            \n",
    "        except torch.cuda.OutOfMemoryError as e:\n",
    "            print(f\"‚ùå CUDA OOM during validation: {e}\")\n",
    "            print(\"üîß Attempting recovery...\")\n",
    "            \n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            \n",
    "            # Use dummy values to continue\n",
    "            val_loss, val_mIoU = 999.0, 0.0\n",
    "            print(\"‚ö†Ô∏è  Using dummy validation values due to memory constraints\")\n",
    "        \n",
    "        # === UPDATE TRAINING STATE ===\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        memory_info = monitor_gpu_memory()\n",
    "        \n",
    "        # Record metrics\n",
    "        training_state['training_history']['train_loss'].append(train_loss)\n",
    "        training_state['training_history']['val_loss'].append(val_loss)\n",
    "        training_state['training_history']['val_mIoU'].append(val_mIoU)\n",
    "        training_state['training_history']['learning_rates'].append(current_lr)\n",
    "        training_state['training_history']['iterations'].append(current_iter)\n",
    "        training_state['training_history']['gpu_memory'].append(memory_info['allocated'])\n",
    "        training_state['current_iteration'] = current_iter\n",
    "        training_state['epoch'] = epoch + 1\n",
    "        \n",
    "        # === RESULTS DISPLAY ===\n",
    "        print_epoch_summary(\n",
    "            epoch + 1, train_loss, val_loss, val_mIoU, \n",
    "            current_lr, current_iter, training_state['best_mIoU'], CFG['MAX_ITERATIONS']\n",
    "        )\n",
    "        \n",
    "        # === MODEL CHECKPOINTING ===\n",
    "        is_best = val_mIoU > training_state['best_mIoU']\n",
    "        if is_best:\n",
    "            training_state['best_mIoU'] = val_mIoU\n",
    "            print(f\"üéâ NEW BEST MODEL! mIoU: {val_mIoU:.6f}\")\n",
    "            print(f\"   ‚îî‚îÄ Saving to: {CFG['MODEL_SAVE_PATH']}\")\n",
    "            \n",
    "            # Save best model state dict only (smaller file)\n",
    "            torch.save(model.state_dict(), CFG['MODEL_SAVE_PATH'])\n",
    "        else:\n",
    "            print(f\"   Current best mIoU: {training_state['best_mIoU']:.6f}\")\n",
    "        \n",
    "        # Save checkpoint every 5 epochs\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            checkpoint_path = CFG['MODEL_SAVE_PATH'].replace('.pth', f'_checkpoint_epoch_{epoch+1}.pth')\n",
    "            save_checkpoint(model, optimizer, scheduler, training_state, checkpoint_path, is_best)\n",
    "            print(f\"üíæ Checkpoint saved: {checkpoint_path}\")\n",
    "        \n",
    "        # === CHECK STOPPING CRITERIA ===\n",
    "        if current_iter >= CFG['MAX_ITERATIONS']:\n",
    "            print(f\"\\nüèÅ MAXIMUM ITERATIONS REACHED!\")\n",
    "            print(f\"   ‚îî‚îÄ Completed {current_iter:,} iterations\")\n",
    "            print(f\"   ‚îî‚îÄ Stopping training...\")\n",
    "            break\n",
    "        \n",
    "        # Show progress and memory\n",
    "        progress = (current_iter / CFG['MAX_ITERATIONS']) * 100\n",
    "        remaining_iters = CFG['MAX_ITERATIONS'] - current_iter\n",
    "        print(f\"   Progress: {progress:.1f}% ({remaining_iters:,} iterations remaining)\")\n",
    "        print(f\"   GPU Memory: {memory_info['allocated']:.2f}GB allocated\")\n",
    "        \n",
    "        # Clear memory between epochs\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(f\"\\n‚ö†Ô∏è  Training interrupted by user!\")\n",
    "    print(f\"   ‚îî‚îÄ Current iteration: {current_iter:,}\")\n",
    "    print(f\"   ‚îî‚îÄ Saving current state...\")\n",
    "    \n",
    "    # Save interrupted state\n",
    "    interrupted_path = CFG['MODEL_SAVE_PATH'].replace('.pth', '_interrupted.pth')\n",
    "    save_checkpoint(model, optimizer, scheduler, training_state, interrupted_path)\n",
    "    print(f\"   ‚îî‚îÄ Saved to: {interrupted_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Training error: {e}\")\n",
    "    \n",
    "    # Save error state\n",
    "    error_path = CFG['MODEL_SAVE_PATH'].replace('.pth', '_error.pth')\n",
    "    try:\n",
    "        save_checkpoint(model, optimizer, scheduler, training_state, error_path)\n",
    "        print(f\"   ‚îî‚îÄ Error state saved to: {error_path}\")\n",
    "    except:\n",
    "        print(f\"   ‚îî‚îÄ Could not save error state\")\n",
    "    \n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "# === TRAINING COMPLETION SUMMARY ===\n",
    "print(f\"\\n\" + \"=\" * 80)\n",
    "print(f\"üéØ TRAINING COMPLETED!\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "final_results = {\n",
    "    'total_epochs': training_state.get('epoch', 0),\n",
    "    'total_iterations': current_iter,\n",
    "    'best_mIoU': training_state.get('best_mIoU', 0.0),\n",
    "    'final_lr': optimizer.param_groups[0]['lr'],\n",
    "    'model_path': CFG['MODEL_SAVE_PATH']\n",
    "}\n",
    "\n",
    "print(f\"Final Results:\")\n",
    "for key, value in final_results.items():\n",
    "    if 'mIoU' in key or 'lr' in key:\n",
    "        print(f\"   ‚îî‚îÄ {key}: {value:.6f}\")\n",
    "    elif 'path' in key:\n",
    "        print(f\"   ‚îî‚îÄ {key}: {value}\")\n",
    "    else:\n",
    "        print(f\"   ‚îî‚îÄ {key}: {value:,}\")\n",
    "\n",
    "# Show training history summary\n",
    "if training_state['training_history']['val_mIoU']:\n",
    "    history = training_state['training_history']\n",
    "    print(f\"\\nTraining History:\")\n",
    "    print(f\"   ‚îî‚îÄ Best mIoU: {max(history['val_mIoU']):.6f}\")\n",
    "    print(f\"   ‚îî‚îÄ Final train loss: {history['train_loss'][-1]:.6f}\")\n",
    "    print(f\"   ‚îî‚îÄ Final val loss: {history['val_loss'][-1]:.6f}\")\n",
    "    print(f\"   ‚îî‚îÄ Epochs completed: {len(history['train_loss'])}\")\n",
    "    print(f\"   ‚îî‚îÄ Peak GPU memory: {max(history['gpu_memory']):.2f} GB\")\n",
    "\n",
    "# Final memory cleanup\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    final_memory = monitor_gpu_memory()\n",
    "    print(f\"   ‚îî‚îÄ Final GPU memory: {final_memory['allocated']:.2f}GB allocated\")\n",
    "\n",
    "print(f\"\\n‚úÖ DeepLabv3+ PASCAL VOC 2012 reproduction training complete!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63113435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== RESULTS VISUALIZATION =====\n",
    "\n",
    "def plot_training_curves(training_history, save_path=None):\n",
    "    \"\"\"\n",
    "    Plot comprehensive training curves including loss, mIoU, and learning rate.\n",
    "    \n",
    "    Args:\n",
    "        training_history: Dictionary containing training metrics\n",
    "        save_path: Optional path to save the plots\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('DeepLabv3+ Training Progress', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Training and Validation Loss\n",
    "    axes[0, 0].plot(training_history['train_loss'], label='Training Loss', color='#e74c3c', linewidth=2)\n",
    "    axes[0, 0].plot(training_history['val_loss'], label='Validation Loss', color='#3498db', linewidth=2)\n",
    "    axes[0, 0].set_title('Loss Curves', fontweight='bold')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Validation mIoU\n",
    "    axes[0, 1].plot(training_history['val_mIoU'], label='Validation mIoU', color='#2ecc71', linewidth=2)\n",
    "    axes[0, 1].set_title('mIoU Progress', fontweight='bold')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('mIoU')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Learning Rate Schedule\n",
    "    axes[1, 0].plot(training_history['iterations'], training_history['learning_rates'], \n",
    "                    color='#f39c12', linewidth=2)\n",
    "    axes[1, 0].set_title('Learning Rate Schedule', fontweight='bold')\n",
    "    axes[1, 0].set_xlabel('Iteration')\n",
    "    axes[1, 0].set_ylabel('Learning Rate')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    axes[1, 0].set_yscale('log')\n",
    "    \n",
    "    # Combined Loss and mIoU\n",
    "    ax_loss = axes[1, 1]\n",
    "    ax_miou = ax_loss.twinx()\n",
    "    \n",
    "    line1 = ax_loss.plot(training_history['val_loss'], color='#3498db', linewidth=2, label='Val Loss')\n",
    "    line2 = ax_miou.plot(training_history['val_mIoU'], color='#2ecc71', linewidth=2, label='Val mIoU')\n",
    "    \n",
    "    ax_loss.set_xlabel('Epoch')\n",
    "    ax_loss.set_ylabel('Validation Loss', color='#3498db')\n",
    "    ax_miou.set_ylabel('Validation mIoU', color='#2ecc71')\n",
    "    ax_loss.set_title('Loss vs mIoU', fontweight='bold')\n",
    "    \n",
    "    # Combine legends\n",
    "    lines = line1 + line2\n",
    "    labels = [l.get_label() for l in lines]\n",
    "    ax_loss.legend(lines, labels, loc='center right')\n",
    "    \n",
    "    ax_loss.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"üìä Training curves saved to: {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def visualize_predictions(model, dataset, device, num_samples=4, save_path=None):\n",
    "    \"\"\"\n",
    "    Visualize model predictions on sample images.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model\n",
    "        dataset: Dataset to sample from\n",
    "        device: Device for inference\n",
    "        num_samples: Number of samples to visualize\n",
    "        save_path: Optional path to save visualization\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Color map for PASCAL VOC classes\n",
    "    colors = plt.cm.tab20(np.linspace(0, 1, CFG['NUM_CLASSES']))\n",
    "    \n",
    "    fig, axes = plt.subplots(num_samples, 3, figsize=(15, 4*num_samples))\n",
    "    if num_samples == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(num_samples):\n",
    "            # Get random sample\n",
    "            idx = random.randint(0, len(dataset) - 1)\n",
    "            sample = dataset[idx]\n",
    "            \n",
    "            # Prepare input\n",
    "            image_tensor = sample['image'].unsqueeze(0).to(device)\n",
    "            \n",
    "            # Get prediction\n",
    "            output = model(image_tensor)['out']\n",
    "            prediction = torch.argmax(output, dim=1).squeeze().cpu().numpy()\n",
    "            \n",
    "            # Denormalize image for display\n",
    "            image = sample['image'].cpu().numpy().transpose(1, 2, 0)\n",
    "            image = image * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n",
    "            image = np.clip(image, 0, 1)\n",
    "            \n",
    "            # Ground truth mask\n",
    "            gt_mask = sample['mask'].cpu().numpy()\n",
    "            \n",
    "            # Plot original image\n",
    "            axes[i, 0].imshow(image)\n",
    "            axes[i, 0].set_title(f'Original Image ({sample[\"image_id\"]})', fontweight='bold')\n",
    "            axes[i, 0].axis('off')\n",
    "            \n",
    "            # Plot ground truth\n",
    "            gt_colored = colors[gt_mask]\n",
    "            axes[i, 1].imshow(gt_colored)\n",
    "            axes[i, 1].set_title('Ground Truth', fontweight='bold')\n",
    "            axes[i, 1].axis('off')\n",
    "            \n",
    "            # Plot prediction\n",
    "            pred_colored = colors[prediction]\n",
    "            axes[i, 2].imshow(pred_colored)\n",
    "            axes[i, 2].set_title('Prediction', fontweight='bold')\n",
    "            axes[i, 2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"üñºÔ∏è  Predictions saved to: {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Plot training results if training was completed\n",
    "print(\"üìà RESULTS VISUALIZATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if 'training_state' in locals() and training_state['training_history']['train_loss']:\n",
    "    print(\"Generating training curves...\")\n",
    "    \n",
    "    # Plot training curves\n",
    "    curves_path = os.path.join(CFG['RESULTS_PATH'], 'training_curves.png')\n",
    "    plot_training_curves(training_state['training_history'], curves_path)\n",
    "    \n",
    "    # Display training statistics\n",
    "    history = training_state['training_history']\n",
    "    print(f\"\\nüìä Training Statistics:\")\n",
    "    print(f\"   ‚îî‚îÄ Final train loss: {history['train_loss'][-1]:.6f}\")\n",
    "    print(f\"   ‚îî‚îÄ Final val loss: {history['val_loss'][-1]:.6f}\")\n",
    "    print(f\"   ‚îî‚îÄ Best mIoU: {max(history['val_mIoU']):.6f}\")\n",
    "    print(f\"   ‚îî‚îÄ Final mIoU: {history['val_mIoU'][-1]:.6f}\")\n",
    "    print(f\"   ‚îî‚îÄ mIoU improvement: {history['val_mIoU'][-1] - history['val_mIoU'][0]:.6f}\")\n",
    "    \n",
    "    # Create learning rate visualization\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(history['iterations'], history['learning_rates'], color='#f39c12', linewidth=2)\n",
    "    plt.title('Polynomial Learning Rate Schedule', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Learning Rate')\n",
    "    plt.yscale('log')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    lr_schedule_path = os.path.join(CFG['RESULTS_PATH'], 'lr_schedule.png')\n",
    "    plt.savefig(lr_schedule_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"‚úÖ Visualizations complete!\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No training history available for visualization\")\n",
    "    print(\"   Run the training loop first to generate results\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f684a1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== MODEL TESTING AND INFERENCE =====\n",
    "\n",
    "def load_best_model(model_path, model, device):\n",
    "    \"\"\"\n",
    "    Load the best saved model for inference.\n",
    "    \n",
    "    Args:\n",
    "        model_path: Path to the saved model\n",
    "        model: Model architecture to load weights into\n",
    "        device: Device to load model on\n",
    "        \n",
    "    Returns:\n",
    "        Loaded model ready for inference\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Loading best model from: {model_path}\")\n",
    "        \n",
    "        # Load state dict\n",
    "        if os.path.exists(model_path):\n",
    "            state_dict = torch.load(model_path, map_location=device)\n",
    "            model.load_state_dict(state_dict)\n",
    "            model.eval()\n",
    "            print(\"‚úÖ Best model loaded successfully\")\n",
    "            return model\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è  Model file not found: {model_path}\")\n",
    "            print(\"   Using current model state instead\")\n",
    "            return model\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading model: {e}\")\n",
    "        print(\"   Using current model state instead\")\n",
    "        return model\n",
    "\n",
    "\n",
    "def evaluate_final_model(model, val_loader, device, num_classes, ignore_index, class_names):\n",
    "    \"\"\"\n",
    "    Comprehensive evaluation of the final model with detailed metrics.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model\n",
    "        val_loader: Validation data loader\n",
    "        device: Device for computation\n",
    "        num_classes: Number of classes\n",
    "        ignore_index: Index to ignore\n",
    "        class_names: List of class names\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with detailed evaluation results\n",
    "    \"\"\"\n",
    "    print(\"üîç COMPREHENSIVE MODEL EVALUATION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    total_samples = 0\n",
    "    \n",
    "    print(\"Running inference on validation set...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(val_loader, desc=\"Evaluating\", \n",
    "                    bar_format='{l_bar}{bar:40}{r_bar}{bar:-40b}')\n",
    "        \n",
    "        for batch in pbar:\n",
    "            images = batch['image'].to(device)\n",
    "            masks = batch['mask'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images)['out']\n",
    "            \n",
    "            # Collect predictions and targets\n",
    "            all_predictions.append(outputs.cpu())\n",
    "            all_targets.append(masks.cpu())\n",
    "            \n",
    "            total_samples += images.size(0)\n",
    "            pbar.set_postfix({'Samples': total_samples})\n",
    "    \n",
    "    # Concatenate all batches\n",
    "    all_predictions = torch.cat(all_predictions, dim=0)\n",
    "    all_targets = torch.cat(all_targets, dim=0)\n",
    "    \n",
    "    print(f\"‚úÖ Processed {total_samples:,} samples\")\n",
    "    \n",
    "    # Compute detailed metrics\n",
    "    print(\"\\nComputing detailed metrics...\")\n",
    "    detailed_results = compute_detailed_miou(\n",
    "        all_predictions, all_targets, num_classes, ignore_index, class_names\n",
    "    )\n",
    "    \n",
    "    return detailed_results, all_predictions, all_targets\n",
    "\n",
    "\n",
    "def visualize_class_performance(detailed_results, save_path=None):\n",
    "    \"\"\"\n",
    "    Visualize per-class IoU performance.\n",
    "    \n",
    "    Args:\n",
    "        detailed_results: Results from detailed mIoU computation\n",
    "        save_path: Optional path to save visualization\n",
    "    \"\"\"\n",
    "    # Extract class names and IoU values\n",
    "    class_names = list(detailed_results['per_class_IoU'].keys())\n",
    "    iou_values = list(detailed_results['per_class_IoU'].values())\n",
    "    \n",
    "    # Filter out NaN values for plotting\n",
    "    valid_data = [(name, iou) for name, iou in zip(class_names, iou_values) \n",
    "                  if not np.isnan(iou)]\n",
    "    \n",
    "    if not valid_data:\n",
    "        print(\"‚ö†Ô∏è  No valid class data for visualization\")\n",
    "        return\n",
    "    \n",
    "    valid_names, valid_ious = zip(*valid_data)\n",
    "    \n",
    "    # Create bar plot\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    bars = plt.bar(range(len(valid_names)), valid_ious, color='skyblue', alpha=0.8)\n",
    "    \n",
    "    # Customize plot\n",
    "    plt.title('Per-Class IoU Performance', fontsize=16, fontweight='bold')\n",
    "    plt.xlabel('Classes', fontsize=12)\n",
    "    plt.ylabel('IoU Score', fontsize=12)\n",
    "    plt.xticks(range(len(valid_names)), valid_names, rotation=45, ha='right')\n",
    "    plt.ylim(0, 1)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, iou in zip(bars, valid_ious):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                f'{iou:.3f}', ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    # Add mean line\n",
    "    mean_iou = detailed_results['mIoU']\n",
    "    plt.axhline(y=mean_iou, color='red', linestyle='--', linewidth=2,\n",
    "                label=f'Mean IoU: {mean_iou:.4f}')\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3, axis='y')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"üìä Class performance saved to: {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def create_inference_samples(model, dataset, device, num_samples=6):\n",
    "    \"\"\"\n",
    "    Create inference samples with predictions for visual inspection.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model\n",
    "        dataset: Dataset to sample from\n",
    "        device: Device for inference\n",
    "        num_samples: Number of samples to process\n",
    "    \"\"\"\n",
    "    print(f\"üñºÔ∏è  CREATING INFERENCE SAMPLES\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    # PASCAL VOC color palette\n",
    "    colors = np.array([\n",
    "        [0, 0, 0], [128, 0, 0], [0, 128, 0], [128, 128, 0], [0, 0, 128],\n",
    "        [128, 0, 128], [0, 128, 128], [128, 128, 128], [64, 0, 0], [192, 0, 0],\n",
    "        [64, 128, 0], [192, 128, 0], [64, 0, 128], [192, 0, 128], [64, 128, 128],\n",
    "        [192, 128, 128], [0, 64, 0], [128, 64, 0], [0, 192, 0], [128, 192, 0],\n",
    "        [0, 64, 128]\n",
    "    ]) / 255.0\n",
    "    \n",
    "    fig, axes = plt.subplots(num_samples, 4, figsize=(16, 4*num_samples))\n",
    "    if num_samples == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(num_samples):\n",
    "            # Get sample\n",
    "            idx = random.randint(0, len(dataset) - 1)\n",
    "            sample = dataset[idx]\n",
    "            \n",
    "            # Prepare input\n",
    "            image_tensor = sample['image'].unsqueeze(0).to(device)\n",
    "            \n",
    "            # Get prediction\n",
    "            output = model(image_tensor)['out']\n",
    "            prediction = torch.argmax(output, dim=1).squeeze().cpu().numpy()\n",
    "            \n",
    "            # Denormalize image\n",
    "            image = sample['image'].cpu().numpy().transpose(1, 2, 0)\n",
    "            image = image * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n",
    "            image = np.clip(image, 0, 1)\n",
    "            \n",
    "            # Ground truth\n",
    "            gt_mask = sample['mask'].cpu().numpy()\n",
    "            \n",
    "            # Create colored masks\n",
    "            gt_colored = colors[gt_mask]\n",
    "            pred_colored = colors[prediction]\n",
    "            \n",
    "            # Create overlay\n",
    "            overlay = 0.6 * image + 0.4 * pred_colored\n",
    "            \n",
    "            # Plot all views\n",
    "            axes[i, 0].imshow(image)\n",
    "            axes[i, 0].set_title(f'Image {sample[\"image_id\"]}', fontweight='bold')\n",
    "            axes[i, 0].axis('off')\n",
    "            \n",
    "            axes[i, 1].imshow(gt_colored)\n",
    "            axes[i, 1].set_title('Ground Truth', fontweight='bold')\n",
    "            axes[i, 1].axis('off')\n",
    "            \n",
    "            axes[i, 2].imshow(pred_colored)\n",
    "            axes[i, 2].set_title('Prediction', fontweight='bold')\n",
    "            axes[i, 2].axis('off')\n",
    "            \n",
    "            axes[i, 3].imshow(overlay)\n",
    "            axes[i, 3].set_title('Overlay', fontweight='bold')\n",
    "            axes[i, 3].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save samples\n",
    "    samples_path = os.path.join(CFG['RESULTS_PATH'], 'inference_samples.png')\n",
    "    plt.savefig(samples_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"üì∏ Samples saved to: {samples_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# === FINAL MODEL EVALUATION ===\n",
    "print(\"üéØ FINAL MODEL TESTING AND EVALUATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Load best model\n",
    "if 'model' in locals():\n",
    "    best_model = load_best_model(CFG['MODEL_SAVE_PATH'], model, CFG['DEVICE'])\n",
    "    \n",
    "    # Comprehensive evaluation\n",
    "    if 'val_loader' in locals():\n",
    "        try:\n",
    "            detailed_results, all_preds, all_targets = evaluate_final_model(\n",
    "                best_model, val_loader, CFG['DEVICE'], \n",
    "                CFG['NUM_CLASSES'], CFG['IGNORE_INDEX'], CFG['VOC_CLASSES']\n",
    "            )\n",
    "            \n",
    "            print(f\"\\nüìä FINAL EVALUATION RESULTS\")\n",
    "            print(\"=\" * 60)\n",
    "            print(f\"Overall mIoU: {detailed_results['mIoU']:.6f}\")\n",
    "            print(f\"Valid classes: {detailed_results['valid_classes']}/{detailed_results['total_classes']}\")\n",
    "            \n",
    "            # Show top performing classes\n",
    "            class_ious = detailed_results['per_class_IoU']\n",
    "            valid_classes = {k: v for k, v in class_ious.items() if not np.isnan(v)}\n",
    "            sorted_classes = sorted(valid_classes.items(), key=lambda x: x[1], reverse=True)\n",
    "            \n",
    "            print(f\"\\nTop 5 performing classes:\")\n",
    "            for i, (class_name, iou) in enumerate(sorted_classes[:5]):\n",
    "                print(f\"   {i+1}. {class_name}: {iou:.4f}\")\n",
    "            \n",
    "            # Visualize class performance\n",
    "            performance_path = os.path.join(CFG['RESULTS_PATH'], 'class_performance.png')\n",
    "            visualize_class_performance(detailed_results, performance_path)\n",
    "            \n",
    "            # Create inference samples\n",
    "            if 'val_dataset' in locals():\n",
    "                create_inference_samples(best_model, val_dataset, CFG['DEVICE'])\n",
    "            \n",
    "            print(f\"\\n‚úÖ Final evaluation complete!\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error during final evaluation: {e}\")\n",
    "    \n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Validation data not available for final evaluation\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Model not available for testing\")\n",
    "    print(\"   Run the training sections first\")\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df53b4f9",
   "metadata": {},
   "source": [
    "## üéâ Notebook Ho√†n T·∫•t!\n",
    "\n",
    "### T√≥m t·∫Øt Notebook DeepLabv3+ PASCAL VOC 2012 Reproduction\n",
    "\n",
    "Notebook n√†y cung c·∫•p m·ªôt implementation ho√†n ch·ªânh v√† ch√≠nh x√°c c·ªßa DeepLabv3+ cho semantic segmentation tr√™n PASCAL VOC 2012, bao g·ªìm:\n",
    "\n",
    "#### ‚úÖ **C√°c Component Ch√≠nh:**\n",
    "\n",
    "1. **Environment Setup** - Import libraries v√† seed reproducibility\n",
    "2. **Configuration** - Single source of truth cho t·∫•t c·∫£ hyperparameters\n",
    "3. **Dataset Download** - T·ª± ƒë·ªông download PASCAL VOC 2012 qua kagglehub\n",
    "4. **Data Pipeline** - Augmentation pipeline theo paper specifications\n",
    "5. **Dataset Class** - Custom PyTorch Dataset cho PASCAL VOC\n",
    "6. **Model Architecture** - DeepLabv3+ v·ªõi ResNet-101 backbone\n",
    "7. **Training Components** - SGD optimizer v·ªõi polynomial LR scheduling\n",
    "8. **Metrics** - mIoU implementation cho semantic segmentation\n",
    "9. **Training Loop** - Iteration-based training v·ªõi checkpointing\n",
    "10. **Visualization** - Training curves v√† prediction samples\n",
    "11. **Final Evaluation** - Comprehensive testing v√† per-class analysis\n",
    "\n",
    "#### üéØ **Paper Compliance:**\n",
    "\n",
    "- ‚úÖ **Model**: DeepLabv3+ v·ªõi ResNet-101 backbone (pretrained)\n",
    "- ‚úÖ **Dataset**: PASCAL VOC 2012 (21 classes)\n",
    "- ‚úÖ **Augmentation**: Random scaling (0.5x-2.0x), cropping (513√ó513), horizontal flip\n",
    "- ‚úÖ **Optimizer**: SGD v·ªõi momentum=0.9, weight_decay=0.0001\n",
    "- ‚úÖ **Learning Rate**: Polynomial decay v·ªõi power=0.9, base_lr=0.007\n",
    "- ‚úÖ **Training**: 30,000 iterations (iteration-based, not epoch-based)\n",
    "- ‚úÖ **Loss**: CrossEntropyLoss v·ªõi ignore_index=255\n",
    "- ‚úÖ **Evaluation**: mIoU (mean Intersection over Union)\n",
    "\n",
    "#### üöÄ **Features:**\n",
    "\n",
    "- **T·ª± ƒë·ªông h√≥a ho√†n to√†n**: T·ª´ download dataset ƒë·∫øn visualization\n",
    "- **Progress tracking**: Real-time monitoring v·ªõi tqdm progress bars\n",
    "- **Checkpointing**: Automatic best model saving\n",
    "- **Error handling**: Robust error handling v√† recovery\n",
    "- **Visualization**: Comprehensive plots v√† prediction samples\n",
    "- **Documentation**: Chi ti·∫øt comments v√† docstrings\n",
    "\n",
    "#### üìä **Output:**\n",
    "\n",
    "- Trained DeepLabv3+ model (`.pth` file)\n",
    "- Training curves (loss, mIoU, learning rate)\n",
    "- Per-class IoU analysis\n",
    "- Sample predictions v·ªõi ground truth comparison\n",
    "- Comprehensive evaluation metrics\n",
    "\n",
    "### H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng:\n",
    "\n",
    "1. **Ch·∫°y t·ª´ng cell theo th·ª© t·ª±** - Notebook ƒë∆∞·ª£c thi·∫øt k·∫ø ƒë·ªÉ ch·∫°y sequential\n",
    "2. **Monitor progress** - Training progress s·∫Ω ƒë∆∞·ª£c hi·ªÉn th·ªã real-time\n",
    "3. **Check outputs** - T·∫•t c·∫£ plots v√† results s·∫Ω ƒë∆∞·ª£c saved t·ª± ƒë·ªông\n",
    "4. **Customize parameters** - Modify CFG dictionary ƒë·ªÉ adjust hyperparameters\n",
    "\n",
    "### K·∫øt qu·∫£ mong ƒë·ª£i:\n",
    "\n",
    "- **mIoU**: ~70-75% tr√™n PASCAL VOC 2012 validation set (depends on training iterations)\n",
    "- **Training time**: ~8-12 hours tr√™n single GPU (V100/A100)\n",
    "- **Memory usage**: ~8-10GB GPU memory\n",
    "\n",
    "---\n",
    "\n",
    "**Ch√∫c b·∫°n train th√†nh c√¥ng! üéØ**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
